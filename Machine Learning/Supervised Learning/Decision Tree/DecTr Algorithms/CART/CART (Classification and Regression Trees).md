分类与回归树(classification and regression tree，CART)模型由 Breiman 等人在 1984年提出，是应用广泛的决策树学习方法。CART 同样由特征选择、树的**生成**及**剪枝**组成，既可以用于**分类**也可以用于**回归**。以下将用于分类与回归的树统称为决策树。[[Classification Trees]]，[[Regression Trees]]

CART分类回归树是一种典型的二叉决策树，可以做**分类**或者**回归**。如果待预测结果是**离散型数据**，则CART生成**分类**决策树；如果待预测结果是**连续型数据**，则CART生成**回归**决策树。

CART 是在给定输入随机变量 $X$ 条件下输出随机变量 $Y$ 的条件概率分布的学习方法。CART 假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是'的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。

[[Gini Index]] 基尼系数本身反应了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。 CART 算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分。

CART算法的重要基础包含以下三个方面：
- 决策树生成：二分(Binary Split)：在每次判断过程中，都是对观察变量进行二分。
	基于**训练数据集**生成决策树，生成的决策树要**尽量大**。
	CART算法采用一种二分递归分割的技术，算法总是将当前样本集分割为两个子样本集，使得生成的决策树的每个非叶结点都只有两个分枝，CART二分每个特征（包括标签特征以及连续特征），如果标签特征有3个属性，可以将其中的两个属性归为一类，另一个属性归为一类。因此CART算法生成的决策树是结构简洁的二叉树。因此CART算法适用于样本特征的取值为是或非的场景，对于连续特征的处理则与C4.5算法相似。

- **单变量**分割(Split Based on One Variable)：每次最优划分都是针对单个变量。

- 剪枝 [[Pruning]] 策略：CART算法的**关键点**，也是整个Tree-Based算法的关键步骤。
	用**验证数据集**对已生成的树进行剪枝并选择最优子树，这时用**损失函数最小**作为剪枝的标准。
	剪枝过程特别重要，所以在**最优决策树生成过程**中占有重要地位。有研究表明，剪枝过程的重要性要比树生成过程更为重要，对于不同的划分标准生成的最大树(Maximum Tree)，在剪枝之后都能够保留最重要的属性划分，差别不大。反而是剪枝方法对于最优树的生成更为关键。

#### CART 生成
决策树的生成就是递归地构建 二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用[[Gini Index]] 基尼系数最小化准则，进行特征选择，生成二叉树

#### CART 剪枝
由于CART生成算法是让生成的决策树自然生长，尽量大，所以容易造成过拟合，因此采取剪枝策略避免过拟合。

CART采取Cost-Complexity Pruning(CCP、代价复杂度)剪枝法：
CCP方法包含两个步骤：
1：从原始决策树 $T_0$ 开始生成一个子树序列 $\{T0,T1,T2,\dots,T_n\}$,其中 $T_{i+1}$ 是从 $T_i$ 总产生，$T_n$为根节点
2：通过交叉验证法 [[Cross-validation]] 在独立的验证数据集上对子树序列 $\{T0,T1,T2,\dots,T_n\}$ 进行测试，从中选择最优子树。

