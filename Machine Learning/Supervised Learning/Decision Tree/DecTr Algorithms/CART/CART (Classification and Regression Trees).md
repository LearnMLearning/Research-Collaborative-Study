分类与回归树(classification and regression tree，CART)模型由 Breiman 等人在 1984年提出，是应用广泛的决策树学习方法。CART 同样由特征选择、树的**生成**及**剪枝**组成，既可以用于**分类**也可以用于**回归**。以下将用于分类与回归的树统称为决策树。[[Classification Trees]]，[[Regression Trees]]

CART分类回归树是一种典型的二叉决策树，可以做**分类**或者**回归**。如果待预测结果是**离散型数据**，则CART生成**分类**决策树；如果待预测结果是**连续型数据**，则CART生成**回归**决策树。

CART 是在给定输入随机变量 $X$ 条件下输出随机变量 $Y$ 的条件概率分布的学习方法。CART 假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是'的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。

[[Gini Index]] 基尼系数本身反应了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。 CART 算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分。

CART算法的重要基础包含以下三个方面：
- 决策树生成：二分(Binary Split)：在每次判断过程中，都是对观察变量进行二分。
	基于**训练数据集**生成决策树，生成的决策树要**尽量大**。
	CART算法采用一种二分递归分割的技术，算法总是将当前样本集分割为两个子样本集，使得生成的决策树的每个非叶结点都只有两个分枝，CART二分每个特征（包括标签特征以及连续特征），如果标签特征有3个属性，可以将其中的两个属性归为一类，另一个属性归为一类。因此CART算法生成的决策树是结构简洁的二叉树。因此CART算法适用于样本特征的取值为是或非的场景，对于连续特征的处理则与C4.5算法相似。

- **单变量**分割(Split Based on One Variable)：每次最优划分都是针对单个变量。

- 剪枝 [[Pruning]] 策略：CART算法的**关键点**，也是整个Tree-Based算法的关键步骤。
	用**验证数据集**对已生成的树进行剪枝并选择最优子树，这时用**损失函数最小**作为剪枝的标准。
	剪枝过程特别重要，所以在**最优决策树生成过程**中占有重要地位。有研究表明，剪枝过程的重要性要比树生成过程更为重要，对于不同的划分标准生成的最大树(Maximum Tree)，在剪枝之后都能够保留最重要的属性划分，差别不大。反而是剪枝方法对于最优树的生成更为关键。

#### CART 生成
决策树的生成就是递归地构建 二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用[[Gini Index]] 基尼系数最小化准则，进行特征选择，生成二叉树
###### 回归树的生成
假设 $X$ 与 $Y$ 分别为输入和输出变量，并且 $Y$ 是连续变量，给定训练数据
$$
D = \{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N) \}
$$
考虑如何生成回归树。
一棵回归树对应着输入空间(即特征空间)的一个划分以及在划分的单元上的输出值。 假设已将输入空间划分为 $M$ 个单元 $R_1,R_2,\cdots,R_M$，并且在每个单元 $R_m$ 上有一个固定的输出值 $c_m$，于是回归树模型可表示为
$$
f(x) = \sum_{m=1}^M c_m I(x\in R_m)
$$
当输入空间的划分确定时，可以用平方误差 $\sum_{x_i \in R_m}(y_i - f(x_i))^2$ 来表示回归数对训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。**易知**(?)，单元 $R_m$ 上的 $c_m$ 的最优值 $\hat c_m$ 是 $R_m$ 上所有输入实例 $x_i$ 对应的输出 $y_i$ 的均值，即
$$
\hat c_m = \text{ave} (y_i | x_i \in R_m)
$$
问题是怎样对输入空间进行划分。这里采用启发式的方法，选择第 $j$ 个变量 $x^{(j)}$ 和它取的值 $s$ 作为切分变量 (splitting variable)和切分点 (splitting point)，并定义两个区域
$$
R_1(j,s) = \{x | x^{(j)} \le s\} , R_2 (j,s) = \{x | x^{(j)} >s\}
$$
然后寻找最优切分变量 $j$ 和最优切分点 $s$。具体地，求解
$$
\mathop{\text{min}}_{j,s} \left[\mathop{\text{min}}_{c_1}\sum_{x_i \in R_1(j,s)} (y_i - c_1)^2 + \mathop{\text{min}}_{c_2}\sum_{x_i\in R_2(j,s)}(y_i - c_2)^2 \right]
$$
对固定输入变量 $j$ 可以找到最优切分点。
$$
\hat c_1 = \text{ave} (y_i | x_i \in R_1 (j,s)),\hat c_2 = \text{ave} (y_i|x_i \in R_2(j,s))
$$
遍历所有输入变量，找到最优的切分变量 $j$，构成一个对 $(j,s)$。依此将输入空间划分为两个 区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。这样就生成一棵回归 树。这样的回归树通常称为最小二乘回归树(least squaresregressiontre)，现将算法叙述如下。
###### 算法 (最小二乘回归树生成算法)
输入：训练数据集 $D$。
输出：回归树 $f(x)$。
在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区 域上的输出值，构建二叉决策树：
(1) 选择最优切分变量 $j$ 与切分点 $s$，求解
$$
\mathop{\text{min}}_{j,s} \left[\mathop{\text{min}}_{c_1}\sum_{x_i \in R_1(j,s)} (y_i - c_1)^2 + \mathop{\text{min}}_{c_2}\sum_{x_i\in R_2(j,s)}(y_i - c_2)^2 \right]
$$
遍历变量 $j$，对固定的切分变量 $j$ 扫描切分点 $s$，选择使式子达到最小值的对 $(j,s)$
(2) 用选定的对 $(j,s)$ 划分区域并决定相应的输出值：
$$
R_1(j,s) = \{x | x^{(j)} \le s\} , R_2 (j,s) = \{x | x^{(j)} >s\}
$$
$$
\hat c_m = \frac{1}{N_m} \sum_{x_{i} \in R_m (j,s)}y_i,\, x\in R_m,\,m=1,2
$$
(3) 继续对两个子区域调用步骤 (1) 和步骤 (2)，直至满足停止条件。
(4) 将输入空间划分为 $M$ 个区域 $R_1,R_2\cdots,R_M$，生成决策树:
$$
f(x) = \sum_{m=1}^M \hat c_m I(x\in R_m)
$$
###### 分类树的生成
分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。


#### CART 剪枝
由于CART生成算法是让生成的决策树自然生长，尽量大，所以容易造成过拟合，因此采取剪枝策略避免过拟合。

CART采取Cost-Complexity Pruning(CCP、代价复杂度)剪枝法：
CCP方法包含两个步骤：
1：从原始决策树 $T_0$ 开始生成一个子树序列 $\{T0,T1,T2,\dots,T_n\}$,其中 $T_{i+1}$ 是从 $T_i$ 总产生，$T_n$为根节点
2：通过交叉验证法 [[Cross-validation]] 在独立的验证数据集上对子树序列 $\{T0,T1,T2,\dots,T_n\}$ 进行测试，从中选择最优子树。

