C4.5算法是用于生成决策树的一种经典算法，是[[ID3]]算法的一种延伸和优化。C4.5算法对ID3算法进行了改进 ，改进点主要有：

1. 用信息增益率 [[Information Gain Ratio]] 来选择划分特征，克服了用信息增益选择的不足，**但信息增益率对可取值数目较少的属性有所偏好**；
2. 能够处理离散型和连续型的属性类型，即将**连续型**的属性进行**离散化**处理； 
3. 能够处理具有**缺失属性值**的训练数据；
4. 在构造树的过程中进行**剪枝** [[Pruning]]；

#### 特征选则
特征选择也即选择最优划分属性，从当前数据的特征中选择一个特征作为当前节点的划分标准。 随着划分过程不断进行，希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的“纯度”越来越高。
具体信息增益相关公式见：[[ID3]] [[Information Gain]]

###### 信息增益率
信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，C4.5算法采用信息增益率来选择最优划分属性。增益率公式
$$
\text{Gain Ratio} (D|A) = \frac{\mathrm{IG}(D|A)}{\mathrm{IV}(A)}
$$
$$
\mathrm {IV} (A) = - \sum_{k=1}^K \frac{|D_k|}{|D|} \log_2 \frac{|D_k|}{|D|}
$$
其中 $A=[a_1,a_2,\dots,a_k]$ ，$K$个值。若使用 A 来对样本集 D 进行划分，则会产生 $K$ 个分支节点，其中第 $k$ 个节点包含 D 中所有属性 A 上取值为 $a_k$ 的样本，记为 $D_k$。**通常，属性A的可能取值数越多（即 $K$ 越大），则IV(A)的值通常会越大**。

信息增益率准则对可取值**数目较少**的属性有所偏好。所以，**C4.5算法不是直接选择信息增益率最大的候选划分属性，而是先从候选划分属性中找出 1 信息增益高于平均水平的属性，再从中选择 2 信息增益率最高的**。

#### 对连续特征的处理
当属性类型为**离散型**，无须对数据进行离散化处理；
当属性类型为**连续型**，则需要对数据进行离散化处理。具体思路如下：
具体思路：
1. $m$ 个样本的连续特征 A 有 $m$ 个值，从小到大排列 $a_1,a_2,\dots,a_m$ ，取相邻两样本值的平均数做划分点，一共有 $m-1$ 个，其中第 $i$ 个划分点 $T_i$ 表示为：$T_i = \frac{a_i + a_{i+1}}{2}$。
2. 分别计算以这 $m-1$ 个点作为二元切分点时的信息增益率。选择信息增益率最大的点为该连续特征的最佳切分点。比如取到的信息增益率最大的点为 $a_t$ ，则小于 $a_t$ 的值为类别 1，大于 $a_t$ 的值为类别 2，这样就做到了连续特征的离散化。
**举例**
离散型属性的值是有限的，比如属性天气的值只有三个：晴天、下雨、阴天，可以一一枚举。

而连续性属性的值不再有限，比如工资，有可能是6000，也有可能是6500，还有7000等等。如果将连续性属性的每一个值，当成单独的值来看待，就会生成下面这样的决策树。这个决策树对于我们判断问题来说，没有任何用处。我们需要得到的是，工资小于6000，大于6000小于10000这样的分类，而不需要每一个工资值的分类。

C4.5算法于是将连续的属性进行离散化，离散化策略就是二分法：
![[Pasted image 20240629083208.png]]
以贷款人员的数据集为例，我们来看看具体的计算过程：
![[Pasted image 20240629083317.png]]
年收入从小到大排列：70，95，100，120，125
计算中值T：82.5，97.5，110，122.5
下面计算T取不同值的信息增益
当T= 82.5时：
![[Pasted image 20240629083343.png]]
当T=97.5时：
![[Pasted image 20240629083358.png]]
同样的方法，求出当T=110、T=122.5时的信息增益，具体的计算过程就不再展示了。

最后我们发现当T=97.5时，信息增益最大。也就是说年收入以97.5作为阀值，划分的数据集不确定性最小。

（2）对于第二个问题，ID3算法由于采用的是信息增益，容易倾向于选择取值较多的属性作为节点。改良后的 C4.5 算法采用的是信息增益率，信息增益率 = 信息增益 / 属性熵
公式：
$$
\text{Gain Ratio}(D,a) = \frac{\text{Gain}(D,a)}{IV(a)}
$$
其中属性熵
$$
IV(a) = -\sum_{i=1}^n \frac{D^i}{D}\log_2 \frac{D^i}{D}
$$
当属性有很多值时，虽然信息增益变大了，但是相应的属性熵也会变大。所以最终计算的信息增益率并不是很大。在一定程度上可以避免 ID3 倾向于选择取值较多的属性作为节点的问题。

还是以这个数据集为例：
![[Pasted image 20240629083620.png]]
当属性为天气时，计算的信息增益、信息增益率如下
![[Pasted image 20240629083637.png]]
当属性为温度时，计算的信息增益和信息增益率如下：
![[Pasted image 20240629083841.png]]
我们对比发现，当温度作为条件时，由于温度有三个值（高、中、低），信息增益为0.32，远大于天气作为条件时的信息增益0.19。当使用信息增益率时，温度作为条件算出来的信息增益率为0.20，与天气作为条件的信息增益率0.19非常接近。

但是使用增益率可能产生另外一个问题，就是如果**属性取值数目较少**，我们来想一个比较极端的例子，假如**属性只取一个值**，属性熵就是0。我们知道一个数除以一个接近0的数，会变成无穷大。所以**增益率可能会偏好取值比较少**的属性。因此C4.5采用了一个启发式的算法，先从候选属性中找出高于平均水平的属性，再从高于平均水平的属性中选择增益率最高的属性。

#### 缺失值的处理
ID3算法不能处理缺失值，但C4.5可以处理缺失值。

缺失值涉及到三个问题
	1. 在有缺失值的特征熵如何计算信息增益比？
		 **根据缺失比例，折算信息增益（无缺失值样本所占的比例乘以无缺失值样本子集的信息增益）和信息增益率**
	2. 选定了划分属性，对于该属性上缺失特征的样本如何进行划分？
		**将样本以不同概率同时划分到不同节点中，概率是根据其他非缺失属性的比例来得到的**
	3. 对于新的样本进行分类时，如果测试样本特性有缺失值应该如何判断其类别？
		**走所有分支，计算每个类别的概率，取概率最大的类别赋值给该样本**

对于第一个子问题，某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。

对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9, 4/9。

具体计算方法，可以看看周志华老师的《机器学习》。
![[Pasted image 20240629090859.png]]
第二个问题的具体计算过程
![[Pasted image 20240629091428.png]]
#### 剪枝 [[Pruning]]
**为什么要剪枝？** 因为过拟合的树在泛化能力的表现非常差。

剪枝又分为前剪枝和后剪枝，前剪枝是指在构造树的过程中就知道哪些节点可以剪掉 。 后剪枝是指构造出完整的决策树之后再来考查哪些子树可以剪掉。

**前剪枝** [[Pre-Pruning]]
在节点划分前确定是否继续增长，及早停止增长的主要方法有：
- 节点内数据样本数小于切分最小样本数阈值；
- 所有节点特征都已分裂；
- 节点划分前准确率比划分后准确率高。
前剪枝不仅可以降低过拟合的风险而且还可以减少训练时间，但另一方面它是基于“贪心”策略，会带来欠拟合风险。

**后剪枝** [[Post-Pruning]]
在已经生成的决策树上进行剪枝，从而得到简化版的剪枝决策树。 

C4.5算法采用**悲观剪枝方法**。根据剪枝前后的误判率来判定是否进行子树的修剪， 如果剪枝后与剪枝前相比其误判率是保持或者下降，则这棵子树就可以被替换为一个叶子节点。 因此，不需要单独的剪枝数据集。C4.5 通过训练数据集上的错误分类数量来估算未知样本上的错误率。

把一颗子树（具有多个叶子节点）的剪枝后用一个叶子节点来替代的话，在训练集上的误判率肯定是上升的，但是在新数据上不一定。于是我们需要把子树的误判计算加上一个经验性的惩罚因子。对于一颗叶子节点，它覆盖了 $N$ 个样本，其中有 $E$ 个错误，那么该叶子节点的错误率为 $\frac{E+0.5}{N}$。这个 $0.5$ 就是惩罚因子，那么一颗子树，它有 $L$ 个叶子节点，那么该子树的误判率估计为：
$$
e = \frac{\sum E_i + 0.5 * L}{\sum N_i}
$$
其中，$E_i$ 表示子树的每一个叶子节点的误判样本数量，$L$ 为子树的叶子节点个数，  为每一个叶子节点的样本数量。

这样的话，我们可以看到一颗子树虽然具有多个子节点，但由于加上了惩罚因子，所以子树的误判率计算未必占到便宜。剪枝后内部节点变成了叶子节点，其误判个数 $J$ 也需要加上一个惩罚因子，变成 $J+0.5$。

那么子树是否可以被剪枝就取决于剪枝后的错误 $J+0.5$ 是否在 ($\sum E_i + 0.5 * L$) 的标准误差内。

对于样本的误判率 $e$ ，可以根据经验把它估计成各种各样的分布模型，比如是二项式分布，比如是正态分布。

那么一棵树错误分类一个样本值为 1 ，正确分类一个样本值为 0 ，该树错误分类的概率（误判率）为 $e$ ，$e$ 通过下式来计算
$$
e = \frac{\sum E_i + 0.5 * L}{\sum N_i}
$$
那么树的误判次数就是伯努利分布，我们可以估计出该树的误判次数的均值和标准差：
$$\begin{aligned}
E (子树误判次数) &= Ne\\
std (子树误判次数) &= \sqrt{Ne(1-e)}\\
\end{aligned}$$
把子树替换成叶子节点后，该叶子的误判次数也是一个伯努利分布，因为子树合并为一个叶子节点了，所以，$L=1$，将其代入上面计算误判率的公式中，可以得到叶子节点的误判率为
$$
e = \frac{E+0.5}{N}
$$
因此叶子节点的误判次数均值为
$$
E(叶子节点的误判次数) = Ne
$$
这里采用一种保守的分裂方案，即有足够大的置信度保证分裂后准确率比不分裂时的准确率高时才分裂，否则就不分裂--也就是应该剪枝。

如果要分裂（即不剪枝）至少要保证分裂后的误判数E(子树误判次数)要小于不分裂的误判数E(叶子节点的误判次数)，而且为了保证足够高的置信度，加了一个标准差可以有95%的置信度，所以，要分裂（即不剪枝）需满足如下不等式
$$
E (子树误判次数) + std(子树误判次数) < E (叶子节点的误判次数)
$$
反之就是不分裂，即 **剪枝的条件**：
$$
E (子树误判次数) + std(子树误判次数) \ge E (叶子节点的误判次数)
$$
###### 例子
对T4这棵子树进行后剪枝
![[Pasted image 20240630014456.png|400]]
子树T4的误判率：
$$\begin{aligned}
子树误判率 \,e &= \frac{\sum_{i=1}^3 E_i + 0.5 L}{\sum_{i=1}^3 N_i}\\
&= \frac{(2+3) + 0.5 \times 3}{16}\\
&= 0.40625
\end{aligned}$$
子树T4误判次数的均值和标准差分别为：
$$\begin{aligned}
E(子树误判次数) &= Ne = 16 \times 0.40625 = 6.5\\
std(子树误判次数) &= \sqrt{Ne(1-e)} = 1.96
\end{aligned}$$
若将子树T4替换为一个叶节点后，其误判率为：
$$
叶子结点误判率 = \frac{7+0.5}{16} = 0.46875
$$
则叶子结点误判次数均值为：
$$\begin{aligned}
E(叶子结点误判次数) &= N \cdot 叶子结点错误率\\
& = 16 \cdot 0.46875\\
& = 7.5
\end{aligned}$$
由于
$$
6.5 + 1.96 > 7.5
$$
满足剪枝条件。所以，应该把 T4 的所有子节点全部剪掉，T4变成一个叶子结点。
#### Python 实现
###### 数据集

###### 代码


#### 总结: C4.5 算法优缺点


C4.5算法虽然解决了ID3的一些缺陷，但是其本身也有一些不足：

（1）C4.5生成的是多叉树，一个父节点可以有多个子节点。计算的时候，运算效率没有二叉树高；

（2）C4.5使用了熵模型，里面有大量的对数运算。如果有连续值的属性，还涉及到排序运算，运算量很大。





https://zhuanlan.zhihu.com/p/89902999

https://zhuanlan.zhihu.com/p/139188759

