C4.5算法在[[ID3]]算法上进行了改良。

###### 处理连续值
离散型属性的值是有限的，比如属性天气的值只有三个：晴天、下雨、阴天，可以一一枚举。

而连续性属性的值不再有限，比如工资，有可能是6000，也有可能是6500，还有7000等等。如果将连续性属性的每一个值，当成单独的值来看待，就会生成下面这样的决策树。这个决策树对于我们判断问题来说，没有任何用处。我们需要得到的是，工资小于6000，大于6000小于10000这样的分类，而不需要每一个工资值的分类。

C4.5算法于是将连续的属性进行离散化，离散化策略就是二分法：
![[Pasted image 20240629083208.png]]
以贷款人员的数据集为例，我们来看看具体的计算过程：
![[Pasted image 20240629083317.png]]
年收入从小到大排列：70，95，100，120，125
计算中值T：82.5，97.5，110，122.5
下面计算T取不同值的信息增益
当T= 82.5时：
![[Pasted image 20240629083343.png]]
当T=97.5时：
![[Pasted image 20240629083358.png]]
同样的方法，求出当T=110、T=122.5时的信息增益，具体的计算过程就不再展示了。

最后我们发现当T=97.5时，信息增益最大。也就是说年收入以97.5作为阀值，划分的数据集不确定性最小。

（2）对于第二个问题，ID3算法由于采用的是信息增益，容易倾向于选择取值较多的属性作为节点。改良后的C4.5算法采用的是信息增益率，信息增益率=信息增益/属性熵
公式：
$$
\text{Gain Ratio}(D,a) = \frac{\text{Gain}(D,a)}{IV(a)}
$$
其中属性熵
$$
IV(a) = -\sum_{i=1}^n \frac{D^i}{D}\log_2 \frac{D^i}{D}
$$

当属性有很多值时，虽然信息增益变大了，但是相应的属性熵也会变大。所以最终计算的信息增益率并不是很大。在一定程度上可以避免ID3倾向于选择取值较多的属性作为节点的问题。

还是以这个数据集为例：
![[Pasted image 20240629083620.png]]
当属性为天气时，计算的信息增益、信息增益率如下
![[Pasted image 20240629083637.png]]
当属性为温度时，计算的信息增益和信息增益率如下：
![[Pasted image 20240629083841.png]]
我们对比发现，当温度作为条件时，由于温度有三个值（高、中、低），信息增益为0.32，远大于天气作为条件时的信息增益0.19。当使用信息增益率时，温度作为条件算出来的信息增益率为0.20，与天气作为条件的信息增益率0.19非常接近。

但是使用增益率可能产生另外一个问题，就是如果属性取值数目较少，我们来想一个比较极端的例子，假如属性只取一个值，属性熵就是0。我们知道一个数除以一个接近0的数，会变成无穷大。所以增益率可能会偏好取值比较少的属性。因此C4.5采用了一个启发式的算法，先从候选属性中找出高于平均水平的属性，再从高于平均水平的属性中选择增益率最高的属性。

###### 缺失值的处理
缺失值涉及到两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。

对于第一个子问题，某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。

对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9, 4/9。

具体计算方法，可以看看周志华老师的《机器学习》。
![[Pasted image 20240629090859.png]]



[[Information Gain Ratio]]

https://zhuanlan.zhihu.com/p/89902999

https://zhuanlan.zhihu.com/p/139188759

