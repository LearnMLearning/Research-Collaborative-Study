C4.5算法是用于生成决策树的一种经典算法，是[[ID3]]算法的一种延伸和优化。C4.5算法对ID3算法进行了改进 ，改进点主要有：

1. 用信息增益率 [[Information Gain Ratio]] 来选择划分特征，克服了用信息增益选择的不足，**但信息增益率对可取值数目较少的属性有所偏好**；
2. 能够处理离散型和连续型的属性类型，即将**连续型**的属性进行**离散化**处理； 
3. 能够处理具有**缺失属性值**的训练数据；
4. 在构造树的过程中进行**剪枝** [[Pruning]]；

#### 特征选则
特征选择也即选择最优划分属性，从当前数据的特征中选择一个特征作为当前节点的划分标准。 随着划分过程不断进行，希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的“纯度”越来越高。
具体信息增益相关公式见：[[ID3]] [[Information Gain]]

###### 信息增益率
信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，C4.5算法采用信息增益率来选择最优划分属性。增益率公式
$$
\text{Gain Ratio} (D|A) = \frac{\mathrm{IG}(D|A)}{\mathrm{IV}(A)}
$$
$$
\mathrm {IV} (A) = - \sum_{k=1}^K \frac{|D_k|}{|D|} \log_2 \frac{|D_k|}{|D|}
$$
其中 $A=[a_1,a_2,\dots,a_k]$ ，$K$个值。若使用 A 来对样本集 D 进行划分，则会产生 $K$ 个分支节点，其中第 $k$ 个节点包含 D 中所有属性 A 上取值为 $a_k$ 的样本，记为 $D_k$。**通常，属性A的可能取值数越多（即 $K$ 越大），则IV(A)的值通常会越大**。

信息增益率准则对可取值**数目较少**的属性有所偏好。所以，**C4.5算法不是直接选择信息增益率最大的候选划分属性，而是先从候选划分属性中找出 1 信息增益高于平均水平的属性，再从中选择 2 信息增益率最高的**。

#### 对连续特征的处理
当属性类型为**离散型**，无须对数据进行离散化处理；
当属性类型为**连续型**，则需要对数据进行离散化处理。具体思路如下：
具体思路：
1. $m$ 个样本的连续特征 A 有 $m$ 个值，从小到大排列 $a_1,a_2,\dots,a_m$ ，取相邻两样本值的平均数做划分点，一共有 $m-1$ 个，其中第 $i$ 个划分点 $T_i$ 表示为：$T_i = \frac{a_i + a_{i+1}}{2}$。
2. 分别计算以这 $m-1$ 个点作为二元切分点时的信息增益率。选择信息增益率最大的点为该连续特征的最佳切分点。比如取到的信息增益率最大的点为 $a_t$ ，则小于 $a_t$ 的值为类别 1，大于 $a_t$ 的值为类别 2，这样就做到了连续特征的离散化。
**举例**
离散型属性的值是有限的，比如属性天气的值只有三个：晴天、下雨、阴天，可以一一枚举。

而连续性属性的值不再有限，比如工资，有可能是6000，也有可能是6500，还有7000等等。如果将连续性属性的每一个值，当成单独的值来看待，就会生成下面这样的决策树。这个决策树对于我们判断问题来说，没有任何用处。我们需要得到的是，工资小于6000，大于6000小于10000这样的分类，而不需要每一个工资值的分类。

C4.5算法于是将连续的属性进行离散化，离散化策略就是二分法：
![[Pasted image 20240629083208.png]]
以贷款人员的数据集为例，我们来看看具体的计算过程：
![[Pasted image 20240629083317.png]]
年收入从小到大排列：70，95，100，120，125
计算中值T：82.5，97.5，110，122.5
下面计算T取不同值的信息增益
当T= 82.5时：
![[Pasted image 20240629083343.png]]
当T=97.5时：
![[Pasted image 20240629083358.png]]
同样的方法，求出当T=110、T=122.5时的信息增益，具体的计算过程就不再展示了。

最后我们发现当T=97.5时，信息增益最大。也就是说年收入以97.5作为阀值，划分的数据集不确定性最小。

（2）对于第二个问题，ID3算法由于采用的是信息增益，容易倾向于选择取值较多的属性作为节点。改良后的 C4.5 算法采用的是信息增益率，信息增益率 = 信息增益 / 属性熵
公式：
$$
\text{Gain Ratio}(D,a) = \frac{\text{Gain}(D,a)}{IV(a)}
$$
其中属性熵
$$
IV(a) = -\sum_{i=1}^n \frac{D^i}{D}\log_2 \frac{D^i}{D}
$$
当属性有很多值时，虽然信息增益变大了，但是相应的属性熵也会变大。所以最终计算的信息增益率并不是很大。在一定程度上可以避免 ID3 倾向于选择取值较多的属性作为节点的问题。

还是以这个数据集为例：
![[Pasted image 20240629083620.png]]
当属性为天气时，计算的信息增益、信息增益率如下
![[Pasted image 20240629083637.png]]
当属性为温度时，计算的信息增益和信息增益率如下：
![[Pasted image 20240629083841.png]]
我们对比发现，当温度作为条件时，由于温度有三个值（高、中、低），信息增益为0.32，远大于天气作为条件时的信息增益0.19。当使用信息增益率时，温度作为条件算出来的信息增益率为0.20，与天气作为条件的信息增益率0.19非常接近。

但是使用增益率可能产生另外一个问题，就是如果**属性取值数目较少**，我们来想一个比较极端的例子，假如**属性只取一个值**，属性熵就是0。我们知道一个数除以一个接近0的数，会变成无穷大。所以**增益率可能会偏好取值比较少**的属性。因此C4.5采用了一个启发式的算法，先从候选属性中找出高于平均水平的属性，再从高于平均水平的属性中选择增益率最高的属性。

#### 缺失值的处理
ID3算法不能处理缺失值，但C4.5可以处理缺失值。

缺失值涉及到三个问题
	1. 在有缺失值的特征熵如何计算信息增益比？
		 **根据缺失比例，折算信息增益（无缺失值样本所占的比例乘以无缺失值样本子集的信息增益）和信息增益率**
	2. 选定了划分属性，对于该属性上缺失特征的样本如何进行划分？
		**将样本以不同概率同时划分到不同节点中，概率是根据其他非缺失属性的比例来得到的**
	3. 对于新的样本进行分类时，如果测试样本特性有缺失值应该如何判断其类别？
		**走所有分支，计算每个类别的概率，取概率最大的类别赋值给该样本**

**举例**
![[Pasted image 20240630073527.png]]
在决策树中处理含有缺失值的样本的时候，需要解决两个问题：
- 如何在属性值缺失的情况下进行划分属性的选择？（比如“色泽”这个属性有的样本在该属性上的值是缺失的，那么该如何计算“色泽”的信息增益？）
- 给定划分属性，若样本在该属性上的值是缺失的，那么该如何对这个样本进行划分？（即到底把这个样本划分到哪个结点里？）

训练集 $D$，属性 $a$，$\tilde D$ 表示 $D$ 中在属性 $a$ 上没有缺失值的样本子集 (比如，假设 $a=$ 色泽，则 $\tilde D = \{2,3,4,6,7,8,9,10,11,12,14,15,16,17\}$)。

**对于第一个问题**
我们可以根据属性上没有缺失的样本集 $\tilde D$ 来计算 $a$ 的 Information Gain. 再给 $\tilde D$ 计算出来的值一个权重，就可以表示训练集 $D$ 中属性 $a$ 的优劣。
假定属性 $a$ 有 $V$ 个可取值 $\{a^1,a^2,\dots,a^V\}$，令 $\tilde D^v$ 表示 $\tilde D$ 中在属性 $a$ 上去值为 $a^v$ 的样本子集。
$\tilde D_k$ 表示 $\tilde D$ 中属于第 $k$ 类的样本自己，则显然有 $\tilde D = \cup_{k=1}^{|y|} \tilde D_k,\tilde D = \cup_{v=1}^V \tilde D^v$，$w_x$ 为每个样本 $x$ 的权重。
定义：
$$
\rho = \frac{\sum_{x \in \tilde D}w_x}{\sum_{x \in D} w_x}, \tilde p_k = \frac{\sum_{x\in \tilde D_k}w_x}{\sum_{x\in \tilde D}w_x}\,(1\le k \le |y|), \tilde r_v = \frac{\sum_{x \in \tilde D^v}w_x}{\sum_{x \in \tilde D}w_x}\, (1\le v \le V)
$$
$\rho$ 表示无缺失值样本所占比例，$\tilde p_k$ 表示无缺失值样本中第 $k$ 类所占比例，$\tilde r_v$ 表示无缺失值样本中在属性 $a$ 上取值 $a^v$ 的样本所占比例。则 $\sum_{k=1}^{|y|} \tilde p_k = 1, \sum_{v=1}^V \tilde r_v = 1$.
因此，可以把前面用到的信息增益公式修改一下
$$
\text{Gain} (D,a) = \rho \times \text{Gain} (\tilde D,a) = \rho \times (\text{Ent}(\tilde D) - \sum_{v=1}^V \tilde r_v \text{Ent}(\tilde D^v))
$$
$$
\text{Ent} (\tilde D) = - \sum_{k=1}^{|y|} \tilde p_k \log_2 \tilde p_k
$$
**对于第二个问题**
若 $x$ 在划分属性 $a$ 上的取值未知，则将 $x$ 同时划入所有子节点，只不过此刻要调整该样本 $x$ 的权重值为: $\tilde r_v \cdot w_x$。直观的看，其实就是让同一个样本以不同的概率划入到不同的子节点去。
![[Pasted image 20240630073527.png]]

**举例**
开始时根节点包括训练集 $D$ 中全部 17 个样本，每个样本权重 $w_x=1$。

**色泽：**
该属性上无缺失值的样本子集 $\tilde D=\{2,3,4.6,7,8,9,10,11,12,14,15,16,17\}$共 $14$ 个样本，因此 $\rho = \frac{14}{17}$ 号，其中正样本比例 $\tilde p_1=\frac{6}{14}$，$\tilde p_2 = \frac{8}{14}$。则,
$$
Ent(\tilde D)= -\sum_{k=1}^2 \tilde p_k \log_2 \tilde p_k=- (\frac{6}{14}\log_2 \frac{6}{14} + \frac{8}{14} \log_2 \frac{8}{14})=0.985
$$
$\tilde D^1\{色泽= 青绿\}:(4,6,10,17)$，$\tilde D^2 \{色泽=乌黑\}:(2,3,7,8,9,15)$，$\tilde D^3 \{色泽 = 浅白:(11,12,14,16)$。则，
$$
Ent(\tilde D^1)= \left(\frac 24 \log_2 \frac 24 + \frac 24 \log_2 \frac 24 \right) = 1
$$
$$
Ent(\tilde D^2)= -\left(\frac 46 \log_2 \frac 46 + \frac 26 \log_2 \frac 26 \right)=0.918
$$
$$
Ent(\tilde D^3)=0
$$
$$
\begin{aligned}
\text{Gain}(\tilde D,色泽) &= \tilde Ent(D)-\sum_{v=1}^3 \tilde r_v \,\text{Ent} (\tilde D^v)\\
&= 0.985- \left(\frac{4}{14} \cdot 1 + \frac {6}{14} \cdot 0.918 + \frac 4 {14} \cdot 0\right)=0.306
\end{aligned}$$
则 $\text{Gain} (D, 色泽) = \rho \cdot \text{Gain} (\tilde D, 色泽) = \frac{14} {17} \cdot 0.306 = 0.252$

**根蒂：**
同种方法可得
$$
\text{Gain} (D,\text{根蒂}) = \rho \cdot \text{Gain} (\tilde D , 根蒂) = \frac {15}{17} \cdot 0.194 = 0.171
$$
**敲声：**
同种方法可得
$$
\text{Gain} (D,敲声) = \rho \cdot \text{Gain} (\tilde D , 敲声) = \frac {15}{17} \cdot 0.165 = 0.145
$$
**纹理：**
同种方法可得
$$
\text{Gain} (D,\text{根蒂}) = \rho \cdot \text{Gain} (\tilde D , 根蒂) = \frac {15}{17} \cdot 0.480 = 0.424
$$
**脐部：**
同种方法可得
$$
\text{Gain} (D,\text{根蒂}) = \rho \cdot \text{Gain} (\tilde D , 根蒂) = \frac {15}{17} \cdot 0.328 = 0.289
$$
**触感：**
同种方法可得
$$
\text{Gain} (D,\text{根蒂}) = \rho \cdot \text{Gain} (\tilde D , 根蒂) = \frac {15}{17} \cdot 0.007 = 0.006
$$
**比较发现，“纹理”在所有属性中的信息增益值最大，因此，“纹理”被选为划分属性，用于对根节点进行划分。划分结果为：** **“纹理=稍糊”分支：{7,9,13,14,17}，** **“纹理=清晰”分支：{1,2,3,4,5,6,15}，“纹理=模糊”分支：{11,12,16}。** 如下图所示：
![[Pasted image 20240630082250.png]]



#### 剪枝 [[Pruning]]
**为什么要剪枝？** 因为过拟合的树在泛化能力的表现非常差。

剪枝又分为前剪枝和后剪枝，前剪枝是指在构造树的过程中就知道哪些节点可以剪掉 。 后剪枝是指构造出完整的决策树之后再来考查哪些子树可以剪掉。

**前剪枝** [[Pre-Pruning]]
在节点划分前确定是否继续增长，及早停止增长的主要方法有：
- 节点内数据样本数小于切分最小样本数阈值；
- 所有节点特征都已分裂；
- 节点划分前准确率比划分后准确率高。
前剪枝不仅可以降低过拟合的风险而且还可以减少训练时间，但另一方面它是基于“贪心”策略，会带来欠拟合风险。

**后剪枝** [[Post-Pruning]]
在已经生成的决策树上进行剪枝，从而得到简化版的剪枝决策树。 

C4.5算法采用**悲观剪枝方法**。根据剪枝前后的误判率来判定是否进行子树的修剪， 如果剪枝后与剪枝前相比其误判率是保持或者下降，则这棵子树就可以被替换为一个叶子节点。 因此，不需要单独的剪枝数据集。C4.5 通过训练数据集上的错误分类数量来估算未知样本上的错误率。

把一颗子树（具有多个叶子节点）的剪枝后用一个叶子节点来替代的话，在训练集上的误判率肯定是上升的，但是在新数据上不一定。于是我们需要把子树的误判计算加上一个经验性的惩罚因子。对于一颗叶子节点，它覆盖了 $N$ 个样本，其中有 $E$ 个错误，那么该叶子节点的错误率为 $\frac{E+0.5}{N}$。这个 $0.5$ 就是惩罚因子，那么一颗子树，它有 $L$ 个叶子节点，那么该子树的误判率估计为：
$$
e = \frac{\sum E_i + 0.5 * L}{\sum N_i}
$$
其中，$E_i$ 表示子树的每一个叶子节点的误判样本数量，$L$ 为子树的叶子节点个数，  为每一个叶子节点的样本数量。

这样的话，我们可以看到一颗子树虽然具有多个子节点，但由于加上了惩罚因子，所以子树的误判率计算未必占到便宜。剪枝后内部节点变成了叶子节点，其误判个数 $J$ 也需要加上一个惩罚因子，变成 $J+0.5$。

那么子树是否可以被剪枝就取决于剪枝后的错误 $J+0.5$ 是否在 ($\sum E_i + 0.5 * L$) 的标准误差内。

对于样本的误判率 $e$ ，可以根据经验把它估计成各种各样的分布模型，比如是二项式分布，比如是正态分布。

那么一棵树错误分类一个样本值为 1 ，正确分类一个样本值为 0 ，该树错误分类的概率（误判率）为 $e$ ，$e$ 通过下式来计算
$$
e = \frac{\sum E_i + 0.5 * L}{\sum N_i}
$$
那么树的误判次数就是伯努利分布，我们可以估计出该树的误判次数的均值和标准差：
$$\begin{aligned}
E (子树误判次数) &= Ne\\
std (子树误判次数) &= \sqrt{Ne(1-e)}\\
\end{aligned}$$
把子树替换成叶子节点后，该叶子的误判次数也是一个伯努利分布，因为子树合并为一个叶子节点了，所以，$L=1$，将其代入上面计算误判率的公式中，可以得到叶子节点的误判率为
$$
e = \frac{E+0.5}{N}
$$
因此叶子节点的误判次数均值为
$$
E(叶子节点的误判次数) = Ne
$$
这里采用一种保守的分裂方案，即有足够大的置信度保证分裂后准确率比不分裂时的准确率高时才分裂，否则就不分裂--也就是应该剪枝。

如果要分裂（即不剪枝）至少要保证分裂后的误判数E(子树误判次数)要小于不分裂的误判数E(叶子节点的误判次数)，而且为了保证足够高的置信度，加了一个标准差可以有95%的置信度，所以，要分裂（即不剪枝）需满足如下不等式
$$
E (子树误判次数) + std(子树误判次数) < E (叶子节点的误判次数)
$$
反之就是不分裂，即 **剪枝的条件**：
$$
E (子树误判次数) + std(子树误判次数) \ge E (叶子节点的误判次数)
$$
###### 例子
对T4这棵子树进行后剪枝
![[Pasted image 20240630014456.png|400]]
子树T4的误判率：
$$\begin{aligned}
子树误判率 \,e &= \frac{\sum_{i=1}^3 E_i + 0.5 L}{\sum_{i=1}^3 N_i}\\
&= \frac{(2+3) + 0.5 \times 3}{16}\\
&= 0.40625
\end{aligned}$$
子树T4误判次数的均值和标准差分别为：
$$\begin{aligned}
E(子树误判次数) &= Ne = 16 \times 0.40625 = 6.5\\
std(子树误判次数) &= \sqrt{Ne(1-e)} = 1.96
\end{aligned}$$
若将子树T4替换为一个叶节点后，其误判率为：
$$
叶子结点误判率 = \frac{7+0.5}{16} = 0.46875
$$
则叶子结点误判次数均值为：
$$\begin{aligned}
E(叶子结点误判次数) &= N \cdot 叶子结点错误率\\
& = 16 \cdot 0.46875\\
& = 7.5
\end{aligned}$$
由于
$$
6.5 + 1.96 > 7.5
$$
满足剪枝条件。所以，应该把 T4 的所有子节点全部剪掉，T4变成一个叶子结点。
#### Python 实现
###### 数据集
数据集的属性有3个，分别是有房情况，婚姻状况和年收入，其中有房情况和婚姻状况是离散的取值，而年收入是连续的取值。拖欠贷款者属于分类的结果。
![[Pasted image 20240630073037.png]]
###### 代码
```python
from math import log
import operator
import numpy as np

def createDataSet():
    """构建数据集"""
    dataSet = [['是', '单身', 125, '否'],
               ['否', '已婚', 100, '否'],
               ['否', '单身', 70, '否'],
               ['是', '已婚', 120, '否'],
               ['否', '离异', 95, '是'],
               ['否', '已婚', 60, '否'],
               ['是', '离异', 220, '否'],
               ['否', '单身', 85, '是'],
               ['否', '已婚', 75, '否'],
               ['否', '单身', 90, '是']]
    labels = ['是否有房', '婚姻状况', '年收入(k)']  # 三个特征
    return dataSet, labels

def calcShannonEnt(dataSet):
    """
    计算给定数据集的香农熵
    :param dataSet:给定的数据集
    :return:返回香农熵
    """
    numEntries = len(dataSet)
    labelCounts ={}
    for featVec in dataSet:
        currentLabel = featVec[-1]
        if currentLabel not in labelCounts.keys():
            labelCounts[currentLabel] =0
        labelCounts[currentLabel] += 1
    shannonEnt = 0.0
    for label in labelCounts.keys():
        prob = float(labelCounts[label])/numEntries
        shannonEnt -= prob*log(prob,2)
    return shannonEnt

def majorityCnt(classList):
    """获取出现次数最好的分类名称"""
    classCount = {}
    classList= np.mat(classList).flatten().A.tolist()[0]  # 数据为[['否'], ['是'], ['是']], 转换后为['否', '是', '是']
    for vote in classList:
        if vote not in classCount.keys():
            classCount[vote] = 0
        classCount[vote] += 1
    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)
    return sortedClassCount[0][0]

def splitDataSet(dataSet,axis,value):
    """对离散型特征划分数据集"""
    retDataSet = []  # 创建新的list对象，作为返回的数据
    for featVec in dataSet:
        if featVec[axis] == value:
            reducedFeatVec = featVec[:axis]
            reducedFeatVec.extend(featVec[axis+1:])  # 抽取
            retDataSet.append(reducedFeatVec)
    return retDataSet


def splitContinuousDataSet(dataSet, axis, value, direction):
    """对连续型特征划分数据集"""
    subDataSet = []
    for featVec in dataSet:
        if direction == 0:
            if featVec[axis] > value:  # 按照大于(>)该值进行划分
                reduceData = featVec[:axis]
                reduceData.extend(featVec[axis + 1:])
                subDataSet.append(reduceData)
        if direction == 1:
            if featVec[axis] <= value:  # 按照小于等于(<=)该值进行划分
                reduceData = featVec[:axis]
                reduceData.extend(featVec[axis + 1:])
                subDataSet.append(reduceData)
    return subDataSet

def chooseBestFeatureToSplit(dataSet, labels):
    """选择最好的数据集划分方式"""
    baseEntropy = calcShannonEnt(dataSet)
    baseGainRatio = 0.0
    bestFeature = -1
    numFeatures = len(dataSet[0]) - 1
    # 建立一个字典，用来存储每一个连续型特征所对应最佳切分点的具体值
    bestSplitDic = {}
    # print('dataSet[0]:' + str(dataSet[0]))
    for i in range(numFeatures):
        # 获取第i个特征的特征值
        featVals = [example[i] for example in dataSet]
        # 如果该特征时连续型数据
        if type(featVals[0]).__name__ == 'float' or type(
                featVals[0]).__name__ == 'int':
            # 将该特征的所有值按从小到大顺序排序
            sortedFeatVals = sorted(featVals)
            # 取相邻两样本值的平均数做划分点，共有 len(featVals)-1 个
            splitList = []
            for j in range(len(featVals) - 1):
                splitList.append(
                    (sortedFeatVals[j] + sortedFeatVals[j + 1]) / 2.0)
            # 遍历每一个切分点
            for j in range(len(splitList)):
                # 计算该划分方式的条件信息熵newEntropy
                newEntropy = 0.0
                value = splitList[j]
                # 将数据集划分为两个子集
                greaterSubDataSet = splitContinuousDataSet(dataSet, i, value, 0)
                smallSubDataSet = splitContinuousDataSet(dataSet, i, value, 1)
                prob0 = len(greaterSubDataSet) / float(len(dataSet))
                newEntropy += prob0 * calcShannonEnt(greaterSubDataSet)
                prob1 = len(smallSubDataSet) / float(len(dataSet))
                newEntropy += prob1 * calcShannonEnt(smallSubDataSet)
                # 计算该划分方式的分裂信息
                splitInfo = 0.0
                splitInfo -= prob0 * log(prob0, 2)
                splitInfo -= prob1 * log(prob1, 2)
                # 计算信息增益率 = 信息增益 / 该划分方式的分裂信息
                gainRatio = float(baseEntropy - newEntropy) / splitInfo
                if gainRatio > baseGainRatio:
                    baseGainRatio = gainRatio
                    bestSplit = j
                    bestFeature = i
            bestSplitDic[labels[i]] = splitList[bestSplit]  # 最佳切分点
        else:  # 如果该特征时连续型数据
            uniqueVals = set(featVals)
            splitInfo = 0.0
            # 计算每种划分方式的条件信息熵newEntropy
            newEntropy = 0.0
            for value in uniqueVals:
                subDataSet = splitDataSet(dataSet, i, value)
                prob = len(subDataSet)/float(len(dataSet))
                splitInfo -= prob * log(prob, 2)  # 计算分裂信息
                newEntropy += prob * calcShannonEnt(subDataSet)  # 计算条件信息熵
            # 若该特征的特征值都相同，说明信息增益和分裂信息都为0，则跳过该特征
            if splitInfo == 0.0:
                continue
            # 计算信息增益率 = 信息增益 / 该划分方式的分裂信息
            gainRatio = float(baseEntropy - newEntropy) / splitInfo
            if gainRatio > baseGainRatio:
                bestFeature = i
                baseGainRatio = gainRatio
    # 如果最佳切分特征是连续型，则最佳切分点为具体的切分值
    if type(dataSet[0][bestFeature]).__name__ == 'float' or type(
            dataSet[0][bestFeature]).__name__ == 'int':
        bestFeatValue = bestSplitDic[labels[bestFeature]]
    # 如果最佳切分特征时离散型，则最佳切分点为 切分特征名称,【其实对于离散型特征这个值没有用】
    if type(dataSet[0][bestFeature]).__name__ == 'str':
        bestFeatValue = labels[bestFeature]
    # print('bestFeature:' + str(labels[bestFeature]) + ', bestFeatValue:' + str(bestFeatValue))
    return bestFeature, bestFeatValue


def createTree(dataSet, labels):
    """创建C4.5树"""
    classList = [example[-1] for example in dataSet]
    # 如果类别完全相同，则停止继续划分
    if classList.count(classList[0]) == len(classList):
        return classList[0]
    # 遍历完所有特征时返回出现次数最多的类别
    if len(dataSet[0]) == 1:
        return majorityCnt(classList)
    bestFeature, bestFeatValue = chooseBestFeatureToSplit(dataSet, labels)
    if bestFeature == -1:  # 如果无法选出最优分类特征，返回出现次数最多的类别
        return majorityCnt(classList)
    bestFeatLabel = labels[bestFeature]
    myTree = {bestFeatLabel: {}}
    subLabels = labels[:bestFeature]
    subLabels.extend(labels[bestFeature + 1:])
    # 针对最佳切分特征是离散型
    if type(dataSet[0][bestFeature]).__name__ == 'str':
        featVals = [example[bestFeature] for example in dataSet]
        uniqueVals = set(featVals)
        for value in uniqueVals:
            reduceDataSet = splitDataSet(dataSet, bestFeature, value)
            # print('reduceDataSet:' + str(reduceDataSet))
            myTree[bestFeatLabel][value] = createTree(reduceDataSet, subLabels)
            # print(myTree[bestFeatLabel][value])
    # 针对最佳切分特征是连续型
    if type(dataSet[0][bestFeature]).__name__ == 'int' or type(
            dataSet[0][bestFeature]).__name__ == 'float':
        # 将数据集划分为两个子集，针对每个子集分别建树
        value = bestFeatValue
        greaterSubDataSet = splitContinuousDataSet(dataSet, bestFeature, value, 0)
        smallSubDataSet = splitContinuousDataSet(dataSet, bestFeature, value, 1)
        # print('greaterDataset:' + str(greaterSubDataSet))
        # print('smallerDataSet:' + str(smallSubDataSet))
        # 针对连续型特征，在生成决策的模块，修改划分点的标签，如“> x.xxx”，"<= x.xxx"
        myTree[bestFeatLabel]['>' + str(value)] = createTree(greaterSubDataSet,subLabels)
        myTree[bestFeatLabel]['<=' + str(value)] = createTree(smallSubDataSet,subLabels)
    return myTree

if __name__ == '__main__':
    dataSet, labels = createDataSet()
    mytree = createTree(dataSet, labels)
    print("最终构建的C4.5分类树为：\n",mytree)
```
运行结果如下
```text
最终构建的C4.5分类树为：
 {'年收入(k)': {'>97.5': '否', '<=97.5': {'婚姻状况': {'离异': '是', '单身': '是', '已婚': '否'}}}}
```
将构建的C4.5分类树绘制出来，用ID3算法里介绍的绘制树形图
![[Pasted image 20240630073207.png]]
#### 总结: C4.5 算法优缺点
**优点：** 产生的分类规则易于理解，准确率较高
**缺点：**
1. C4.5算法只能用于分类；
2. C4.5是多叉树，用二叉树效率会提高；
3. 在构造树的过程中，需要对数据集进行多次的顺序扫描和排序（尤其是对连续特征），因而导致算法的低效； 
4. 在选择分裂属性时没有考虑到条件属性间的相关性，只计算数据集中每一个条件属性与决策属性之间的期望信息，有可能影响到属性选择的正确性；
5. C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行；

C4.5算法虽然解决了ID3的一些缺陷，但是其本身也有一些不足：

（1）C4.5生成的是多叉树，一个父节点可以有多个子节点。计算的时候，运算效率没有二叉树高；

（2）C4.5使用了熵模型，里面有大量的对数运算。如果有连续值的属性，还涉及到排序运算，运算量很大。





https://zhuanlan.zhihu.com/p/89902999

https://zhuanlan.zhihu.com/p/139188759

https://blog.csdn.net/u012328159/article/details/79413610