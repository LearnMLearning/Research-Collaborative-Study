#### 基本介绍
###### 基尼指数
分类树用基尼指数 [[Gini Index]] 最小化准则进行特征选择生成二叉树 ，基尼指数 $G(D,A)$ 表示经 $A=a$分割后集合 $D$ 的不确定性，基尼指数越大，样本集合的不确定性就越大。
$$
G(S) = 1-\sum_{i=1}^np_i^2
$$
**注意**：只要是分类性质的样本集，本身就存在基尼指数这个属性。

###### 基尼增益
$$
\text{Gini gain} = \sum_i \left(\frac{N_i}{N} G(S) \right)
$$
Gini 增益表示当前属性的混乱度。其中 $\frac{N_i}{N}$ 表示当前类别占所有类别的概率
**最终Cart分类树选择GiniGain最小的特征作为划分特征。**
【Cart分类树选择GiniGain最小的特征作为划分特征】这里怎么理解呢？
其实就是我们用每个特征（这里是A）的每个可能的值（这里是a）进行二分，每分一次就会产生两份样本，然后按照上式求每一次的综合基尼指数，最后选择基尼指数最小的那个分法就好了。
$$
\text{Gini} (D,A) = \frac{|D_1|}{|D|} \text{Gini}(D_1) + \frac{|D_2|}{|D|} \text{Gini} (D_2),D_1 = \{(x,y) \in D | A(x) = a\},D_2 = D - D_1
$$
**注意**：CART树的每一步都是二分，没有多分情况。
#### 实现步骤
1. 根据当前特征计算他们的基尼增益
2. 选择基尼增益最小的特征作为划分特征
3. 从该特征中查找基尼指数最小的分类类别作为最优划分点。
4. 将当前样本划分成两类，一类是划分特征的类别等于最优划分点，另一类就是不等于
针对这两类递归进行上述的划分工作，直达结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值（样本基本属于同一类），或者没有更多特征。


https://blog.csdn.net/u012856866/article/details/138860813