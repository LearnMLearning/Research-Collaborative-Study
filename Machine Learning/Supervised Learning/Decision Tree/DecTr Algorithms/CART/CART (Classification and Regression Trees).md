[[Gini Index]] 基尼系数本身反应了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。 CART 算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分。

CART分类回归树是一种典型的二叉决策树，可以做**分类**或者**回归**。如果待预测结果是**离散型数据**，则CART生成**分类**决策树；如果待预测结果是**连续型数据**，则CART生成**回归**决策树。

CART算法的重要基础包含以下三个方面：
- 二分(Binary Split)：在每次判断过程中，都是对观察变量进行二分。
	CART算法采用一种二分递归分割的技术，算法总是将当前样本集分割为两个子样本集，使得生成的决策树的每个非叶结点都只有两个分枝，CART二分每个特征（包括标签特征以及连续特征），如果标签特征有3个属性，可以将其中的两个属性归为一类，另一个属性归为一类。因此CART算法生成的决策树是结构简洁的二叉树。因此CART算法适用于样本特征的取值为是或非的场景，对于连续特征的处理则与C4.5算法相似。

- **单变量**分割(Split Based on One Variable)：每次最优划分都是针对单个变量。

- 剪枝 [[Pruning]] 策略：CART算法的**关键点**，也是整个Tree-Based算法的关键步骤。
	剪枝过程特别重要，所以在**最优决策树生成过程**中占有重要地位。有研究表明，剪枝过程的重要性要比树生成过程更为重要，对于不同的划分标准生成的最大树(Maximum Tree)，在剪枝之后都能够保留最重要的属性划分，差别不大。反而是剪枝方法对于最优树的生成更为关键。

[[Classification Trees]]

[[Regression Trees]]

###### 剪枝
由于CART生成算法是让生成的决策树自然生长，尽量大，所以容易造成过拟合，因此采取剪枝策略避免过拟合。

CART采取Cost-Complexity Pruning(CCP、代价复杂度)剪枝法：
CCP方法包含两个步骤：
1：从原始决策树 $T_0$ 开始生成一个子树序列 $\{T0,T1,T2,\dots,T_n\}$,其中 $T_{i+1}$ 是从 $T_i$ 总产生，$T_n$为根节点
2：通过交叉验证法 [[Cross-validation]] 在独立的验证数据集上对子树序列 $\{T0,T1,T2,\dots,T_n\}$ 进行测试，从中选择最优子树。

