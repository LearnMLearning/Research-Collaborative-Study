<?xml version="1.0"?>
<html xmlns="http://www.w3.org/1999/xhtml"><head><link href="style.css" rel="stylesheet" type="text/css"/><title>LIBSVM Data: Classification (Binary Class)</title><meta charset="UTF-8"/></head><body><a name="#top"/><h1><a href="../../libsvm">LIBSVM</a> Data: Classification (Binary Class)</h1><p><font size="+1">
    This page contains many classification, regression,
    multi-label and string data sets stored in LIBSVM format. For some sets
    raw materials (e.g., original texts) are also available. These data sets
    are from UCI, Statlog, StatLib and other collections. We
    thank their efforts. For most sets, we linearly scale each attribute to [-1,1] or [0,1].  The testing data (if provided)
    is adjusted accordingly. Some training data are further separated 
    to "training" (tr) and "validation" (val) sets. Details can be
    found in the description of each data set. To read data via MATLAB, you can use "libsvmread" in LIBSVM package.
  </font></p><hr/><a name="a1a"><h2>a1a</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / Adult</li><li>Preprocessing:
               The original Adult data set has 14 features, among which six 
		are continuous and eight are categorical. In this data set, 
		continuous features are discretized into quantiles, and
		each quantile is represented by a binary feature.
		Also, a categorical feature with m categories is converted to m binary features. 
		Details on how each feature is converted can be found in the beginning of each file 
		from <a href="http://research.microsoft.com/en-us/um/people/jplatt/adult.zip">this page</a>.
                [<a href="ref.html#JP98a">JP98a</a>]
              </li><li># of classes: 2</li><li># of data:
            1,605
                  / 30,956 (testing)
                </li><li># of features:
            123
                  / 123 (testing)
                </li><li>Files:
            <ul><li><a href="binary/a1a">a1a</a></li><li><a href="binary/a1a.t">a1a.t</a> (testing)</li></ul></li></ul><a name="a2a"><h2>a2a</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / Adult</li><li>Preprocessing:
               The same as a1a.
                [<a href="ref.html#JP98a">JP98a</a>]
              </li><li># of classes: 2</li><li># of data:
            2,265
                  / 30,296 (testing)
                </li><li># of features:
            123
                  / 123 (testing)
                </li><li>Files:
            <ul><li><a href="binary/a2a">a2a</a></li><li><a href="binary/a2a.t">a2a.t</a> (testing)</li></ul></li></ul><a name="a3a"><h2>a3a</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / Adult</li><li>Preprocessing:
               The same as a1a.
                [<a href="ref.html#JP98a">JP98a</a>]
              </li><li># of classes: 2</li><li># of data:
            3,185
                  / 29,376 (testing)
                </li><li># of features:
            123
                  / 123 (testing)
                </li><li>Files:
            <ul><li><a href="binary/a3a">a3a</a></li><li><a href="binary/a3a.t">a3a.t</a> (testing)</li></ul></li></ul><a name="a4a"><h2>a4a</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / Adult</li><li>Preprocessing:
               The same as a1a. 
                [<a href="ref.html#JP98a">JP98a</a>]
              </li><li># of classes: 2</li><li># of data:
            4,781
                  / 27,780 (testing)
                </li><li># of features:
            123
                  / 123 (testing)
                </li><li>Files:
            <ul><li><a href="binary/a4a">a4a</a></li><li><a href="binary/a4a.t">a4a.t</a> (testing)</li></ul></li></ul><a name="a5a"><h2>a5a</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / Adult</li><li>Preprocessing:
               The same as a1a. 
                [<a href="ref.html#JP98a">JP98a</a>]
              </li><li># of classes: 2</li><li># of data:
            6,414
                  / 26,147 (testing)
                </li><li># of features:
            123
                  / 123 (testing)
                </li><li>Files:
            <ul><li><a href="binary/a5a">a5a</a></li><li><a href="binary/a5a.t">a5a.t</a> (testing)</li></ul></li></ul><a name="a6a"><h2>a6a</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / Adult</li><li>Preprocessing:
               The same as a1a. 
                [<a href="ref.html#JP98a">JP98a</a>]
              </li><li># of classes: 2</li><li># of data:
            11,220
                  / 21,341 (testing)
                </li><li># of features:
            123
                  / 123 (testing)
                </li><li>Files:
            <ul><li><a href="binary/a6a">a6a</a></li><li><a href="binary/a6a.t">a6a.t</a> (testing)</li></ul></li></ul><a name="a7a"><h2>a7a</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / Adult</li><li>Preprocessing:
               The same as a1a. 
                [<a href="ref.html#JP98a">JP98a</a>]
              </li><li># of classes: 2</li><li># of data:
            16,100
                  / 16,461 (testing)
                </li><li># of features:
            123
                  / 123 (testing)
                </li><li>Files:
            <ul><li><a href="binary/a7a">a7a</a></li><li><a href="binary/a7a.t">a7a.t</a> (testing)</li></ul></li></ul><a name="a8a"><h2>a8a</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / Adult</li><li>Preprocessing:
               The same as a1a. 
                [<a href="ref.html#JP98a">JP98a</a>]
              </li><li># of classes: 2</li><li># of data:
            22,696
                  / 9,865 (testing)
                </li><li># of features:
            123
                  / 123 (testing)
                </li><li>Files:
            <ul><li><a href="binary/a8a">a8a</a></li><li><a href="binary/a8a.t">a8a.t</a> (testing)</li></ul></li></ul><a name="a9a"><h2>a9a</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / Adult</li><li>Preprocessing:
               The same as a1a. 
                [<a href="ref.html#JP98a">JP98a</a>]
              </li><li># of classes: 2</li><li># of data:
            32,561
                  / 16,281 (testing)
                </li><li># of features:
            123
                  / 123 (testing)
                </li><li>Files:
            <ul><li><a href="binary/a9a">a9a</a></li><li><a href="binary/a9a.t">a9a.t</a> (testing)</li></ul></li></ul><a name="australian"><h2>australian</h2></a><ul><li>Source:
            <a href="http://www.liacc.up.pt/ML/old/statlog/datasets.html">Statlog</a>
              / Australian</li><li># of classes: 2</li><li># of data:
            690</li><li># of features:
            14</li><li>Files:
            <ul><li><a href="binary/australian">australian</a></li><li><a href="binary/australian_scale">australian_scale</a> (scaled to [-1,1])</li></ul></li></ul><a name="avazu"><h2>avazu</h2></a><ul><li>Source:
            <a href="https://www.kaggle.com/c/avazu-ctr-prediction/data">Avazu's Click-through Prediction</a></li><li>Preprocessing:
              
        This data is used in a <a href="https://www.kaggle.com/c/avazu-ctr-prediction/">competition</a> on click-through rate prediction jointly hosted by Avazu and Kaggle in 2014.
        The participants were asked to learn a model from the first 10 days of advertising log, and predict the click probability for the impressions on the 11th day.
        The data sets here are generated by applying <a href="http://github.com/guestwalk/kaggle-avazu">our winning solution</a> without some complicated components.
        To reproduce this data, you can execute <a href="http://github.com/guestwalk/kaggle-avazu">our code</a> and see the results in the directory "base."
        For better test scores, we divide the data to two disjoint groups "app" and "site," and conduct training and prediction tasks on the two groups independently.
        Specifically, each instance has either "site_id=85f751fd" or "app_id=ecad2386," and these two feature values never co-occur.
        Thus we can split the data set according to them.
        The organizers do not disclose the test labels, so the labels in the test sets are <b>not meaningful</b>.
        To obtain a test score, please use the code provided below to generate and submit a file to <a href="https://www.kaggle.com/c/avazu-ctr-prediction">the competition site</a>.
        Because data are timely dependent, cross validation is not suitable for parameter selection.
        We provide a training-validation split (e.g., "avazu-app.tr" and "avazu-app.val") by consider the last 4,218,938 training instances for validation.
	
                [<a href="ref.html#YJ16a">YJ16a</a>]
              </li><li># of classes: 2</li><li># of data:
            40,428,967
                  / 4,577,464 (testing)
                
                  / 14,596,137 (avazu-app)
                
                  / 1,719,304 (avazu-app.t)
                
                  / 12,642,186 (avazu-app.tr)
                
                  / 1,953,951 (avazu-app.val)
                
                  / 25,832,830 (avazu-site)
                
                  / 2,858,160 (avazu-site.t)
                
                  / 23,567,843 (avazu-site.tr)
                
                  / 2,264,987 (avazu-site.val)
                </li><li># of features:
            1,000,000</li><li>Files:
            <ul><li><a href="binary/avazu-app.bz2">avazu-app.bz2</a> (app)</li><li><a href="binary/avazu-app.t.bz2">avazu-app.t.bz2</a> (app's testing)</li><li><a href="binary/avazu-app.tr.bz2">avazu-app.tr.bz2</a> (app's tr)</li><li><a href="binary/avazu-app.val.bz2">avazu-app.val.bz2</a> (app's val)</li><li><a href="binary/avazu-site.bz2">avazu-site.bz2</a> (site)</li><li><a href="binary/avazu-site.t.bz2">avazu-site.t.bz2</a> (site's testing)</li><li><a href="binary/avazu-site.tr.bz2">avazu-site.tr.bz2</a> (site's tr)</li><li><a href="binary/avazu-site.val.bz2">avazu-site.val.bz2</a> (site's val)</li><li><a href="binary/avazu-submit.zip">avazu-submit.zip</a> (code to generate a submission file)</li></ul></li></ul><a name="breast-cancer"><h2>breast-cancer</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / Wisconsin Breast Cancer</li><li>Preprocessing:
               Note that the original data has the column 1 containing sample ID. Also 16 instances with missing values are removed.</li><li># of classes: 2</li><li># of data:
            683</li><li># of features:
            10</li><li>Files:
            <ul><li><a href="binary/breast-cancer">breast-cancer</a></li><li><a href="binary/breast-cancer_scale">breast-cancer_scale</a> (scaled to [-1,1])</li></ul></li></ul><a name="cod-rna"><h2>cod-rna</h2></a><ul><li>Source:
            
              [<a href="ref.html#AVU06a">AVU06a</a>]
            </li><li>Features:
               
	<ol>
		<li>Divide by 10 to get deltaG_total value computed by the Dynalign algorithm</li>
		<li>The length of shorter sequence</li>
		<li>'A' frequencies of sequence 1</li>
		<li>'U' frequencies of sequence 1</li>
		<li>'C' frequencies of sequence 1</li>
		<li>'A' frequencies of sequence 2</li>
		<li>'U' frequencies of sequence 2</li>
		<li>'C' frequencies of sequence 2</li>
	</ol> 
    </li><li># of classes: 2</li><li># of data:
            59,535
                  / 271617 (validation)
                
                  / 157413 (unused/remaining)
                </li><li># of features:
            8</li><li>Files:
            <ul><li><a href="binary/cod-rna">cod-rna</a> (training)</li><li><a href="binary/cod-rna.t">cod-rna.t</a> (validation)</li><li><a href="binary/cod-rna.r">cod-rna.r</a> (unused/remaining)</li></ul></li></ul><a name="colon-cancer"><h2>colon-cancer</h2></a><ul><li>Source:
             
              [<a href="ref.html#AU99a">AU99a</a>]
            </li><li>Preprocessing:
              Instance-wise normalization to mean zero and variance one. Then feature-wise normalization to mean zero and variance one.
                [<a href="ref.html#SKS03a">SKS03a</a>]
              </li><li># of classes: 2</li><li># of data:
            62</li><li># of features:
            2,000</li><li>Files:
            <ul><li><a href="binary/colon-cancer.bz2">colon-cancer.bz2</a></li></ul></li></ul><a name="covtype.binary"><h2>covtype.binary</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / Covertype</li><li>Preprocessing:
              Transform from multiclass into binary class.
                [<a href="ref.html#RC02a">RC02a</a>]
              </li><li># of classes: 2</li><li># of data:
            581,012</li><li># of features:
            54</li><li>Files:
            <ul><li><a href="binary/covtype.libsvm.binary.bz2">covtype.libsvm.binary.bz2</a></li><li><a href="binary/covtype.libsvm.binary.scale.bz2">covtype.libsvm.binary.scale.bz2</a> (scaled to [0,1])</li></ul></li></ul><a name="criteo"><h2>criteo</h2></a><ul><li>Source:
            <a href="https://ailab.criteo.com/ressources/">Criteo's Display Advertising Challenge</a></li><li>Preprocessing:
              
        This data is used in a <a href="https://www.kaggle.com/c/criteo-display-ad-challenge/">competition</a> on click-through rate prediction jointly hosted by Criteo and Kaggle in 2014.
        The script for transforming data to LIBFFM and LIBSVM formats is provided in the link down below.
        The features are generated based on a simplified version of the winning solution.
        Please download the scripts here and check the README file for details.
        We also provide code to generate submission files for evaluation at the competition site.
	
                [<a href="ref.html#YJ16a">YJ16a</a>]
              </li><li># of classes: 2</li><li># of data:
            45,840,617
                  / 6,042,135 (testing)
                </li><li># of features:
            1,000,000</li><li>Files:
            <ul><li><a href="http://go.criteo.net/criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz">criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz</a> (link to raw data at criteo)</li><li><a href="binary/criteo.kaggle2014.svm.tar.xz">criteo.kaggle2014.svm.tar.xz</a> (LIBSVM format)</li><li><a href="binary/criteo_tb_trans.zip">criteo_tb_trans.zip</a> (code to generate data in LIBSVM format)</li></ul></li></ul><a name="criteo_tb"><h2>criteo_tb</h2></a><ul><li>Source:
            <a href="https://ailab.criteo.com/criteo-1tb-click-logs-dataset/">Criteo's Terabyte Click Logs</a></li><li>Preprocessing:
              
The original data are click logs of 24 days and we made the first 23 days as the training set and the last day as the testing set.
The features are generated based on a simplified version of the winning solution of a smaller-scaled <a href="https://www.kaggle.com/c/criteo-display-ad-challenge/">competition</a> on click-through rate prediction jointly hosted by Criteo and Kaggle in 2014.
The script for transforming data to LIBFFM and LIBSVM formats is provided in the link down below.
It is same as the one for the
<a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#criteo">criteo</a>
data set.
If you don't have enough RAM to run LIBLINEAR,
you can use the following 
<a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#large_linear_classification_when_data_cannot_fit_in_memory">code</a>
at LIBSVM tools and see our experimental log
<a href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/docs/criteo_tb/criteo_tb_document.pdf">here</a>.
The code used is a disk-level linear classifier.
</li><li># of classes: 2</li><li># of data:
            4,195,197,692
                  / 178,274,637 (testing)
                </li><li># of features:
            1,000,000</li><li>Files:
            <ul><li><a href="binary/criteo_tb.svm.tar.xz">criteo_tb.svm.tar.xz</a> (LIBSVM format)</li><li><a href="binary/criteo_tb_trans.zip">criteo_tb_trans.zip</a> (code to generate data)</li><li><a href="binary/logloss.py">logloss.py</a> (code to calculate log loss)</li></ul></li></ul><a name="diabetes"><h2>diabetes</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / Pima Indians Diabetes</li><li># of classes: 2</li><li># of data:
            768</li><li># of features:
            8</li><li>Files:
            <ul><li><a href="binary/diabetes">diabetes</a></li><li><a href="binary/diabetes_scale">diabetes_scale</a> (scaled to [-1,1])</li></ul></li></ul><a name="duke breast-cancer"><h2>duke breast-cancer</h2></a><ul><li>Source:
             
              [<a href="ref.html#MW01a">MW01a</a>]
            </li><li>Preprocessing:
              Instance-wise normalization to mean zero and variance one. Then feature-wise normalization to mean zero and variance one. The original dataset consists of 49 instances. Five are removed since the classification results using immunohistochemistry and protein immunoblotting assay conflicted. Of the remaining, two instances were rejected due to failed array hybridization. The rest data are further split into training (38), and validation (4).
                [<a href="ref.html#SKS03a">SKS03a</a>]
              </li><li># of classes: 2</li><li># of data:
            44</li><li># of features:
            7,129</li><li>Files:
            <ul><li><a href="binary/duke.bz2">duke.bz2</a></li><li><a href="binary/duke.tr.bz2">duke.tr.bz2</a> (tr)</li><li><a href="binary/duke.val.bz2">duke.val.bz2</a> (val)</li></ul></li></ul><a name="epsilon"><h2>epsilon</h2></a><ul><li>Source:
            <a href="http://largescale.ml.tu-berlin.de/instructions/">PASCAL Challenge 2008</a></li><li>Preprocessing:
              The raw data set (epsilon_train) is instance-wisely scaled to unit length and split into two parts: 4/5 for training and 1/5 for testing. The training part is feature-wisely normalized to mean zero and variance one and then instance-wisely scaled to unit length. Using the scaling factors of the training part, the testing part is processed in a similar way. These train and testing data sets are used in 
                [<a href="ref.html#GXY11a">GXY11a</a>]
              </li><li># of classes: 2</li><li># of data:
            400,000
                  / 100,000 (testing)
                </li><li># of features:
            2,000</li><li>Files:
            <ul><li><a href="binary/epsilon_normalized.bz2">epsilon_normalized.bz2</a></li><li><a href="binary/epsilon_normalized.t.bz2">epsilon_normalized.t.bz2</a> (testing)</li></ul></li></ul><a name="fourclass"><h2>fourclass</h2></a><ul><li>Source:
             
              [<a href="ref.html#TKH96a">TKH96a</a>]
            </li><li>Preprocessing:
              transform to two-class</li><li># of classes: 2</li><li># of data:
            862</li><li># of features:
            2</li><li>Files:
            <ul><li><a href="binary/fourclass">fourclass</a></li><li><a href="binary/fourclass_scale">fourclass_scale</a> (scaled to [-1,1])</li></ul></li></ul><a name="german.numer"><h2>german.numer</h2></a><ul><li>Source:
            <a href="http://www.liacc.up.pt/ML/old/statlog/datasets.html">Statlog</a>
              / German</li><li># of classes: 2</li><li># of data:
            1,000</li><li># of features:
            24</li><li>Files:
            <ul><li><a href="binary/german.numer">german.numer</a></li><li><a href="binary/german.numer_scale">german.numer_scale</a> (scaled to [-1,1])</li></ul></li></ul><a name="gisette"><h2>gisette</h2></a><ul><li>Source:
            <a href="http://www.nipsfsc.ecs.soton.ac.uk/datasets/">NIPS 2003 Feature Selection Challenge</a>
              [<a href="ref.html#IG05a">IG05a</a>]
            </li><li>Preprocessing:
              
	The data set is also available at <a href="https://archive.ics.uci.edu/ml/datasets/Gisette">UCI</a>.
	Because the labels of testing set are not available, here we use the validation
        set (gisette_valid.data and gisette_valid.labels) as the testing set.
	The training data (gisette_train) are feature-wisely scaled to [-1,1]. Then the testing data (gisette_valid) are scaled based on the same scaling factors for the training data. These two scaled data sets are used in 
                [<a href="ref.html#GXY11a">GXY11a</a>]
              </li><li># of classes: 2</li><li># of data:
            6,000
                  / 1,000 (testing)
                </li><li># of features:
            5,000</li><li>Files:
            <ul><li><a href="binary/gisette_scale.bz2">gisette_scale.bz2</a></li><li><a href="binary/gisette_scale.t.bz2">gisette_scale.t.bz2</a> (testing)</li></ul></li></ul><a name="heart"><h2>heart</h2></a><ul><li>Source:
            <a href="http://www.liacc.up.pt/ML/old/statlog/datasets.html">Statlog</a>
              / Heart</li><li># of classes: 2</li><li># of data:
            270</li><li># of features:
            13</li><li>Files:
            <ul><li><a href="binary/heart">heart</a></li><li><a href="binary/heart_scale">heart_scale</a> (scaled to [-1,1])</li></ul></li></ul><a name="HIGGS"><h2>HIGGS</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / HIGGS</li><li>Preprocessing:
              In the original paper, the last 500,000 instances are used for testing, while the remaining are for training. See 
                [<a href="ref.html#PB14a">PB14a</a>]
              </li><li># of classes: 2</li><li># of data:
            11,000,000</li><li># of features:
            28</li><li>Files:
            <ul><li><a href="binary/HIGGS.xz">HIGGS.xz</a></li></ul></li></ul><a name="Hyperpartisan News Detection"><h2>Hyperpartisan News Detection</h2></a><ul><li>Source:
            <a href="https://aclanthology.org/S19-2145/">
      SemEval-2019 Task 4: Hyperpartisan News Detection
    </a></li><li>Preprocessing:
              
      The original dataset can be downloaded from <a href="https://zenodo.org/record/1489920">Zenodo</a>.
      The texts and labels are stored in
      articles-training-byarticle-20181122.zip and ground-truth-training-byarticle-20181122.zip respectively.
      We followed the preprocessing procedures from
      <a href="https://github.com/allenai/longformer/blob/master/scripts/hp_preprocess.py">Longformer</a>.
      We provide both a full-size dataset and training/validation/test subsets that we split according to
      <a href="https://github.com/allenai/longformer/blob/master/scripts/hp-splits.json">hp-splits.json</a>.
    </li><li># of classes: 2</li><li># of data:
            516
                  / 64 (validation)
                
                  / 65 (testing)
                </li><li># of features:
            </li><li>Files:
            <ul><li><a href="binary/generate_hyperpartisan.py">generate_hyperpartisan.py</a></li><li><a href="binary/hyperpartisan_raw_texts.txt.bz2">hyperpartisan_raw_texts.txt.bz2</a></li><li><a href="binary/hyperpartisan_raw_texts_train.txt.bz2">hyperpartisan_raw_texts_train.txt.bz2</a></li><li><a href="binary/hyperpartisan_raw_texts_val.txt.bz2">hyperpartisan_raw_texts_val.txt.bz2</a> (val)</li><li><a href="binary/hyperpartisan_raw_texts_test.txt.bz2">hyperpartisan_raw_texts_test.txt.bz2</a> (testing)</li></ul></li></ul><a name="ijcnn1"><h2>ijcnn1</h2></a><ul><li>Source:
             
              [<a href="ref.html#DP01a">DP01a</a>]
            </li><li>Preprocessing:
              We use winner's transformation
                [<a href="ref.html#Chang01d">Chang01d</a>]
              </li><li># of classes: 2</li><li># of data:
            49,990
                  / 91,701 (testing)
                </li><li># of features:
            22</li><li>Files:
            <ul><li><a href="binary/ijcnn1.bz2">ijcnn1.bz2</a></li><li><a href="binary/ijcnn1.t.bz2">ijcnn1.t.bz2</a> (testing)</li><li><a href="binary/ijcnn1.tr.bz2">ijcnn1.tr.bz2</a> (tr)</li><li><a href="binary/ijcnn1.val.bz2">ijcnn1.val.bz2</a> (val)</li></ul></li></ul><a name="imdb-sentiment"><h2>imdb-sentiment</h2></a><ul><li>Source:
            <a href="https://aclanthology.org/P11-1015/">
      Learning Word Vectors for Sentiment Analysis
    </a></li><li>Preprocessing:
              
      The original dataset can be downloaded from
      <a href="https://ai.stanford.edu/~amaas/data/sentiment/">Large Movie Review Dataset</a>.
      It has already been split into training and test datasets.
      We replaced any sequence of whitespace characters \s (a shorthand for [ \t\n\r\f\v]) with a space.
    </li><li># of classes: 2</li><li># of data:
            25,000
                  / 25,000 (testing)
                </li><li># of features:
            </li><li>Files:
            <ul><li><a href="binary/generate_imdb_sentiment.py">generate_imdb_sentiment.py</a></li><li><a href="binary/imdb_sentiment_raw_texts_train.txt.bz2">imdb_sentiment_raw_texts_train.txt.bz2</a></li><li><a href="binary/imdb_sentiment_raw_texts_test.txt.bz2">imdb_sentiment_raw_texts_test.txt.bz2</a> (testing)</li></ul></li></ul><a name="ionosphere"><h2>ionosphere</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / Ionosphere</li><li># of classes: 2</li><li># of data:
            351</li><li># of features:
            34</li><li>Files:
            <ul><li><a href="binary/ionosphere_scale">ionosphere_scale</a> (scaled to [-1,1])</li></ul></li></ul><a name="kdd2010 (algebra)"><h2>kdd2010 (algebra)</h2></a><ul><li>Source:
            <a href="https://pslcdatashop.web.cmu.edu/KDDCup/downloads.jsp">KDD CUP 2010</a></li><li>Preprocessing:
               
KDD Cup 2010 is an educational data mining competition. The data comes from Carnegie Learning and DataShop.
This is the training set of the first problem: algebra_2008_2009.
We provide a transformed version used by the winner (National Taiwan Univ). Because labels of
the competition's testing set are not available, the training
data is split to two sets for training and validation. The validation set is called the testing 
set here.
To access the raw data set, please check the above "KDD CUP 2010" link.
This data set is only to be used for research purposes. Users please acknowledge the data is from Carnegie Learning and DataShop.
	
                [<a href="ref.html#HFY10c">HFY10c</a>]
              </li><li># of classes: 2</li><li># of data:
            8,407,752
                  / 510,302 (testing)
                </li><li># of features:
            20,216,830
                  / 20,216,830 (testing)
                </li><li>Files:
            <ul><li><a href="binary/kdda.bz2">kdda.bz2</a></li><li><a href="binary/kdda.t.bz2">kdda.t.bz2</a> (testing)</li></ul></li></ul><a name="kdd2010 (bridge to algebra)"><h2>kdd2010 (bridge to algebra)</h2></a><ul><li>Source:
            <a href="https://pslcdatashop.web.cmu.edu/KDDCup/downloads.jsp">KDD CUP 2010</a></li><li>Preprocessing:
               
KDD Cup 2010 is an educational data mining competition. The data comes from Carnegie Learning and DataShop.
This is the training set of the second problem: bridge_to_algebra_2008_2009.
We provide a transformed version used by the winner (National Taiwan Univ). Because labels of
the competition's testing set are not available, the training
data is split to two sets for training and validation. The validation set is called the testing 
set here.
To access the raw data set, please check the above "KDD CUP 2010" link.
This data set is only to be used for research purposes. Users please acknowledge the data is from Carnegie Learning and DataShop.
	
                [<a href="ref.html#HFY10c">HFY10c</a>]
              </li><li># of classes: 2</li><li># of data:
            19,264,097
                  / 748,401 (testing)
                </li><li># of features:
            29,890,095
                  / 29,890,095 (testing)
                </li><li>Files:
            <ul><li><a href="binary/kddb.bz2">kddb.bz2</a></li><li><a href="binary/kddb.t.bz2">kddb.t.bz2</a> (testing)</li></ul></li></ul><a name="kdd2010 raw version (bridge to algebra)"><h2>kdd2010 raw version (bridge to algebra)</h2></a><ul><li>Source:
            <a href="https://pslcdatashop.web.cmu.edu/KDDCup/downloads.jsp">KDD CUP 2010</a></li><li>Preprocessing:
              
        This data set comes from the same source as "kdd2010 (bridge to algebra)."
        All settings are the same except that we give raw data without applying the winner's feature engineering procedure.
        We provide data in the original format and in the LIBSVM format.
        For the LIBSVM-format data, we treat each feature as a categorical type and use binary encoding to generate a sparse feature vector.
        This set was used in experiments in [<a href="ref.html#CWH01a">YJ16a</a>].
	</li><li># of classes: 2</li><li># of data:
            19,264,097
                  / 748,401 (testing)
                </li><li># of features:
            1,163,024</li><li>Files:
            <ul><li><a href="binary/kddb-raw.bz2">kddb-raw.bz2</a> (raw)</li><li><a href="binary/kddb-raw.t.bz2">kddb-raw.t.bz2</a> (raw: testing)</li><li><a href="binary/kddb-raw-libsvm.bz2">kddb-raw-libsvm.bz2</a> (libsvm)</li><li><a href="binary/kddb-raw-libsvm.t.bz2">kddb-raw-libsvm.t.bz2</a> (libsvm: testing)</li></ul></li></ul><a name="kdd2012"><h2>kdd2012</h2></a><ul><li>Source:
            <a href="http://www.kddcup2012.org/c/kddcup2012-track2">KDD CUP 2012</a></li><li>Preprocessing:
              
        We generate this data set from the official "training.txt" file of the second track in KDD CUP 2012.
        In the given file, each line gives a feature vector and the number of clicked/non-clicked impressions under these feature values.
        We set the label to be positive if the number of clicks is non-zero and negative otherwise.
        Every feature is treated as categorical and converted to binary features according to the number of possible categories.
        In addition, each feature vector is normalized to have unit length.
        Because the official evaluation system no longer works, we also provide a 80-20 split used in our paper for calculating the test score.
	
                [<a href="ref.html#YJ16a">YJ16a</a>]
              </li><li># of classes: 2</li><li># of data:
            149,639,105
                  / 119,705,032 (training)
                
                  / 29,934,073 (validation)
                </li><li># of features:
            54,686,452</li><li>Files:
            <ul><li><a href="binary/kdd12.xz">kdd12.xz</a></li><li><a href="binary/kdd12.tr.xz">kdd12.tr.xz</a> (tr)</li><li><a href="binary/kdd12.val.xz">kdd12.val.xz</a> (val)</li></ul></li></ul><a name="leukemia"><h2>leukemia</h2></a><ul><li>Source:
             
              [<a href="ref.html#TG99a">TG99a</a>]
            </li><li>Preprocessing:
              Merge training/testing. Instance-wise normalization to mean zero and variance one. Then feature-wise normalization to mean zero and variance one.
                [<a href="ref.html#SKS03a">SKS03a</a>]
              </li><li># of classes: 2</li><li># of data:
            38
                  / 34 (testing)
                </li><li># of features:
            7129</li><li>Files:
            <ul><li><a href="binary/leu.bz2">leu.bz2</a></li><li><a href="binary/leu.t.bz2">leu.t.bz2</a> (testing)</li></ul></li></ul><a name="liver-disorders"><h2>liver-disorders</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / Liver-disorders</li><li>Preprocessing:
              
	The original data set has 7 variables per instance.  The last variable is a selector indicating whether
	an instance goes to training or testing data set.
	Previously, the data set was wrongly interpreted by using the last variable as the label. Since May 21, 2016, we have followed the
	recommendation made by James McDermott and the data set donor Richard S. Forsyth to address the issue.
	Now the label of an instance is determined by the 6th variable:
	if the 6th variable is larger than 3, than the label is 1; otherwise it's 0.    
                [<a href="ref.html#JM16a">JM16a</a>]
              </li><li># of classes: 2</li><li># of data:
            145
                  / 200 (testing)
                </li><li># of features:
            5</li><li>Files:
            <ul><li><a href="binary/liver-disorders">liver-disorders</a></li><li><a href="binary/liver-disorders_scale">liver-disorders_scale</a> (scaled to [-1,1])</li><li><a href="binary/liver-disorders.t">liver-disorders.t</a> (testing)</li></ul></li></ul><a name="madelon"><h2>madelon</h2></a><ul><li>Source:
            <a href="http://www.nipsfsc.ecs.soton.ac.uk/datasets/">NIPS 2003 Feature Selection Challenge</a>
              [<a href="ref.html#IG05a">IG05a</a>]
            </li><li>Preprocessing:
              
	The data set is also available at <a href="https://archive.ics.uci.edu/ml/datasets/Madelon">UCI</a>.
	Because the labels of testing set are not available, here we use the validation
	set (madelon_valid.data and madelon_valid.labels) as the testing set.
    </li><li># of classes: 2</li><li># of data:
            2,000
                  / 600 (testing)
                </li><li># of features:
            500</li><li>Files:
            <ul><li><a href="binary/madelon">madelon</a></li><li><a href="binary/madelon.t">madelon.t</a> (testing)</li></ul></li></ul><a name="mushrooms"><h2>mushrooms</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / mushrooms</li><li>Preprocessing:
               Each nominal attribute is expanded into several binary attributes. The original attribute #12 has missing values and is not used.</li><li># of classes: 2</li><li># of data:
            8124</li><li># of features:
            112</li><li>Files:
            <ul><li><a href="binary/mushrooms">mushrooms</a></li></ul></li></ul><a name="news20.binary"><h2>news20.binary</h2></a><ul><li>Source:
             
              [<a href="ref.html#SSK05a">SSK05a</a>]
            </li><li>Preprocessing:
               Each instance has unit length.</li><li># of classes: 2</li><li># of data:
            19,996</li><li># of features:
            1,355,191</li><li>Files:
            <ul><li><a href="binary/news20.binary.bz2">news20.binary.bz2</a></li></ul></li></ul><a name="phishing"><h2>phishing</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / Phishing Websites</li><li>Preprocessing:
               All features are categorical. We use binary encoding to generate feature vectors. Each feature vector is normalized to maintain unit-length. 
                [<a href="ref.html#YJ16a">YJ16a</a>]
              </li><li># of classes: 2</li><li># of data:
            11,055</li><li># of features:
            68</li><li>Files:
            <ul><li><a href="binary/phishing">phishing</a></li></ul></li></ul><a name="rcv1.binary"><h2>rcv1.binary</h2></a><ul><li>Source:
             
              [<a href="ref.html#DL04b">DL04b</a>]
            </li><li>Preprocessing:
              positive: CCAT, ECAT; negative: GCAT, MCAT; instances in both positive and negative classes are removed.</li><li># of classes: 2</li><li># of data:
            20,242
                  / 677,399 (testing)
                </li><li># of features:
            47,236</li><li>Files:
            <ul><li><a href="binary/rcv1_train.binary.bz2">rcv1_train.binary.bz2</a></li><li><a href="binary/rcv1_test.binary.bz2">rcv1_test.binary.bz2</a> (testing)</li></ul></li></ul><a name="real-sim"><h2>real-sim</h2></a><ul><li>Source:
            <a href="http://www.cs.umass.edu/~mccallum/code-data.html">A. McCallum</a>
              / Real vs. Simulated</li><li>Preprocessing:
              Vikas Sindhwani for the <a href="http://people.cs.uchicago.edu/~vikass/svmlin.html">SVMlin</a> project</li><li># of classes: 2</li><li># of data:
            72,309</li><li># of features:
            20,958</li><li>Files:
            <ul><li><a href="binary/real-sim.bz2">real-sim.bz2</a></li></ul></li></ul><a name="skin_nonskin"><h2>skin_nonskin</h2></a><ul><li>Source:
            <a href="https://archive.ics.uci.edu/ml/datasets/Skin+Segmentation">UCI</a>
              / Skin Segmentation Data Set</li><li># of classes: 2</li><li># of data:
            245,057</li><li># of features:
            3</li><li>Files:
            <ul><li><a href="binary/skin_nonskin">skin_nonskin</a></li></ul></li></ul><a name="splice"><h2>splice</h2></a><ul><li>Source:
            <a href="http://www.cs.toronto.edu/~delve/data/datasets.html">Delve</a>
              / splice</li><li># of classes: 2</li><li># of data:
            1,000
                  / 2,175 (testing)
                </li><li># of features:
            60</li><li>Files:
            <ul><li><a href="binary/splice">splice</a></li><li><a href="binary/splice_scale">splice_scale</a> (scaled to [-1,1])</li><li><a href="binary/splice.t">splice.t</a> (testing)</li></ul></li></ul><a name="splice-site"><h2>splice-site</h2></a><ul><li>Source:
            
              [<a href="ref.html#SS10a,AA12a">SS10a,AA12a</a>]
            
              / splice-site</li><li>Preprocessing:
               
In Sonnenburg and Franc (2010) for splice site prediction,
they internally map data to a high dimensional space and
train a linear classifier.
Agarwal et al. (2014)
use the 
<a href="http://olivier.chapelle.cc/pub/splice_explicit_features.tar">script</a>
to explicitly store all feature values.
We apply the same script to
<a href="http://sonnenburgs.de/soeren/item/coffin/">the
original data</a>
for generating training/test files:

<pre>
> splice_explicit_features data/H_sapiens_acc_all_examples_plain_139-279_50000000.fasta data/H_sapiens_acc_all_examples_plain_139-279_50000000.fasta_down data/H_sapiens_acc_all_examples_plain_139-279_50000000.fasta_up data/H_sapiens_acc_all_examples_plain_50000000.label  
</pre>

and

<pre>
> splice_explicit_features data/H_sapiens_acc_all_examples_plain_139-279_5e7_test.fasta data/H_sapiens_acc_all_examples_plain_139-279_5e7_test.fasta_down data/H_sapiens_acc_all_examples_plain_139-279_5e7_test.fasta_up data/H_sapiens_acc_all_examples_plain_5e7_test.label  
</pre>

This set is highly skewed, so auPRC (area under precision-recall curve)
is the suitable criterion. Using matlab statistics toolbox, you can
obtain auPRC by

<pre>
[Xpr,Ypr,Tpr,AUCpr] = perfcurve(labels, predictions, 1, 'xCrit', 'reca', 'yCrit', 'prec'); AUCpr
</pre>

where labels are true labels and predictions are your predicted decision values.
You can use LIBLINEAR with option -s 3 (i.e., l2-regularized l1-loss SVM)
to get auPRC of 0.5773, similar to 0.5775 reported in Table 2 of
Sonnenburg and Franc (2010). If you don't have enough RAM to run LIBLINEAR,
you can use the following 
<a href="http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#large_linear_classification_when_data_cannot_fit_in_memory">code</a>
at LIBSVM tools and see our experimental log
<a href="http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/docs/splice-site/splice_document.pdf">here</a>.
The code used is a disk-level linear classifier.

                [<a href="ref.html#HFY11a">HFY11a</a>]
              </li><li># of classes: 2</li><li># of data:
            50,000,000
                  / 4,627,840 (testing)
                </li><li># of features:
            11,725,480</li><li>Files:
            <ul><li><a href="binary/splice_site.xz">splice_site.xz</a> (md5sum=df3bd1b65b9df5776907721dff4fdb4e)</li><li><a href="binary/splice_site.t.xz">splice_site.t.xz</a> (testing)</li></ul></li></ul><a name="sonar"><h2>sonar</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / Undocumented/Sonar</li><li># of classes: 2</li><li># of data:
            208</li><li># of features:
            60</li><li>Files:
            <ul><li><a href="binary/sonar_scale">sonar_scale</a> (scaled to [-1,1])</li></ul></li></ul><a name="SUSY"><h2>SUSY</h2></a><ul><li>Source:
            <a href="http://www.ics.uci.edu/~mlearn/MLRepository.html">UCI</a>
              / SUSY</li><li>Preprocessing:
              In the original paper, the last 500,000 instances are used for testing, while the remaining are for training. See 
                [<a href="ref.html#PB14a">PB14a</a>]
              </li><li># of classes: 2</li><li># of data:
            5,000,000</li><li># of features:
            18</li><li>Files:
            <ul><li><a href="binary/SUSY.xz">SUSY.xz</a></li></ul></li></ul><a name="svmguide1"><h2>svmguide1</h2></a><ul><li>Source:
             
              [<a href="ref.html#CWH03a">CWH03a</a>]
            </li><li>Preprocessing:
              Original data: an astroparticle application from Jan Conrad of Uppsala University, Sweden. </li><li># of classes: 2</li><li># of data:
            3,089
                  / 4,000 (testing)
                </li><li># of features:
            4</li><li>Files:
            <ul><li><a href="binary/svmguide1">svmguide1</a></li><li><a href="binary/svmguide1.t">svmguide1.t</a> (testing)</li></ul></li></ul><a name="svmguide3"><h2>svmguide3</h2></a><ul><li>Source:
             
              [<a href="ref.html#CWH03a">CWH03a</a>]
            </li><li>Preprocessing:
              Original data: someone from Germany working with the car industry. </li><li># of classes: 2</li><li># of data:
            1,243
                  / 41 (testing)
                </li><li># of features:
            21</li><li>Files:
            <ul><li><a href="binary/svmguide3">svmguide3</a></li><li><a href="binary/svmguide3.t">svmguide3.t</a> (testing)</li></ul></li></ul><a name="url"><h2>url</h2></a><ul><li>Source:
             
              [<a href="ref.html#JM09a">JM09a</a>]
            </li><li>Preprocessing:
               The file "url_original.tar.bz2" contains a directory  121 days, in which the file "FeatureTypes" gives indices of real-valued features (other features are 0/1). The file "url_combined.bz2" combines all 121-day data into one file. See more details in <a href="http://sysnet.ucsd.edu/projects/url/">this page</a>. </li><li># of classes: 2</li><li># of data:
            2,396,130</li><li># of features:
            3,231,961</li><li>Files:
            <ul><li><a href="binary/url_combined.bz2">url_combined.bz2</a></li><li><a href="binary/url_combined_normalized.bz2">url_combined_normalized.bz2</a> (scaled to unit length for each instance)</li><li><a href="binary/url_original.tar.bz2">url_original.tar.bz2</a></li></ul></li></ul><a name="w1a"><h2>w1a</h2></a><ul><li>Source:
             
              [<a href="ref.html#JP98a">JP98a</a>]
            </li><li># of classes: 2</li><li># of data:
            2,477
                  / 47,272 (testing)
                </li><li># of features:
            300
                  / 300 (testing)
                </li><li>Files:
            <ul><li><a href="binary/w1a">w1a</a></li><li><a href="binary/w1a.t">w1a.t</a> (testing)</li></ul></li></ul><a name="w2a"><h2>w2a</h2></a><ul><li>Source:
             
              [<a href="ref.html#JP98a">JP98a</a>]
            </li><li># of classes: 2</li><li># of data:
            3,470
                  / 46,279 (testing)
                </li><li># of features:
            300
                  / 300 (testing)
                </li><li>Files:
            <ul><li><a href="binary/w2a">w2a</a></li><li><a href="binary/w2a.t">w2a.t</a> (testing)</li></ul></li></ul><a name="w3a"><h2>w3a</h2></a><ul><li>Source:
             
              [<a href="ref.html#JP98a">JP98a</a>]
            </li><li># of classes: 2</li><li># of data:
            4,912
                  / 44,837 (testing)
                </li><li># of features:
            300
                  / 300 (testing)
                </li><li>Files:
            <ul><li><a href="binary/w3a">w3a</a></li><li><a href="binary/w3a.t">w3a.t</a> (testing)</li></ul></li></ul><a name="w4a"><h2>w4a</h2></a><ul><li>Source:
             
              [<a href="ref.html#JP98a">JP98a</a>]
            </li><li># of classes: 2</li><li># of data:
            7,366
                  / 42,383 (testing)
                </li><li># of features:
            300
                  / 300 (testing)
                </li><li>Files:
            <ul><li><a href="binary/w4a">w4a</a></li><li><a href="binary/w4a.t">w4a.t</a> (testing)</li></ul></li></ul><a name="w5a"><h2>w5a</h2></a><ul><li>Source:
             
              [<a href="ref.html#JP98a">JP98a</a>]
            </li><li># of classes: 2</li><li># of data:
            9,888
                  / 39,861 (testing)
                </li><li># of features:
            300
                  / 300 (testing)
                </li><li>Files:
            <ul><li><a href="binary/w5a">w5a</a></li><li><a href="binary/w5a.t">w5a.t</a> (testing)</li></ul></li></ul><a name="w6a"><h2>w6a</h2></a><ul><li>Source:
             
              [<a href="ref.html#JP98a">JP98a</a>]
            </li><li># of classes: 2</li><li># of data:
            17,188
                  / 32,561 (testing)
                </li><li># of features:
            300
                  / 300 (testing)
                </li><li>Files:
            <ul><li><a href="binary/w6a">w6a</a></li><li><a href="binary/w6a.t">w6a.t</a> (testing)</li></ul></li></ul><a name="w7a"><h2>w7a</h2></a><ul><li>Source:
             
              [<a href="ref.html#JP98a">JP98a</a>]
            </li><li># of classes: 2</li><li># of data:
            24,692
                  / 25,057 (testing)
                </li><li># of features:
            300
                  / 300 (testing)
                </li><li>Files:
            <ul><li><a href="binary/w7a">w7a</a></li><li><a href="binary/w7a.t">w7a.t</a> (testing)</li></ul></li></ul><a name="w8a"><h2>w8a</h2></a><ul><li>Source:
             
              [<a href="ref.html#JP98a">JP98a</a>]
            </li><li># of classes: 2</li><li># of data:
            49,749
                  / 14,951 (testing)
                </li><li># of features:
            300
                  / 300 (testing)
                </li><li>Files:
            <ul><li><a href="binary/w8a">w8a</a></li><li><a href="binary/w8a.t">w8a.t</a> (testing)</li></ul></li></ul><a name="webspam"><h2>webspam</h2></a><ul><li>Source:
            <a href="http://www.cc.gatech.edu/projects/doi/WebbSpamCorpus.html">Webb Spam Corpus</a>
              [<a href="ref.html#ST06a">ST06a</a>]
            </li><li>Preprocessing:
              We consider the subset used in the <a href="http://largescale.first.fraunhofer.de/instructions/">Pascal Large Scale Learning Challenge</a>.
According to Soeren Sonnenburg, 
all positive examples were taken and the negative examples were created
by randomly traversing the Internet starting at well known (e.g. news) web-sites.
We treat continuous n bytes as a word: trigram if n = 3 and unigram if n = 1.
We use word count as the feature value 
and normalize each instance to unit length.
For unigram, the number of features is 254.
Contact us if you need scripts to obtain the
data from the original documents.
Note that the trigram version contains only
680,715 nonzero feature columns.

</li><li># of classes: 2</li><li># of data:
            350,000</li><li># of features:
            16,609,143</li><li>Files:
            <ul><li><a href="binary/webspam_wc_normalized_trigram.svm.xz">webspam_wc_normalized_trigram.svm.xz</a> (Tri-gram)</li><li><a href="binary/webspam_wc_normalized_unigram.svm.xz">webspam_wc_normalized_unigram.svm.xz</a> (Uni-gram)</li></ul></li></ul></body></html>
