Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Membership inference attacks against machine learning models. In IEEE Symposium on Security and Privacy. IEEE Computer Society, 3–18. DOI:http://dx.doi.org/10.1109/SP.2017.41

https://github.com/yonsei-sslab/MIA

**Shadow training**
train the attack model on the labeled inputs and outputs

我们开发了几种有效的方法来生成阴影模型的训练数据。第一种方法使用黑盒访问目标模型来合成这些数据。第二种方法使用关于目标训练数据集抽取的总体的统计数据。第三种方法假设对手可以访问目标训练数据集的潜在噪声版本。第一种方法不假设目标模型的训练数据分布有任何先验知识，而第二种和第三种方法允许攻击者在推断给定记录是否在其训练数据集中之前只查询目标模型一次。

为了训练我们的攻击模型，我们建立了多个“影子”模型，它们的行为与目标模型相似。与目标模型相比，我们知道每个阴影模型的基本真相，即给定记录是否在其训练数据集中。因此，我们可以对影子模型的输入和相应的输出(每个标记为“in”或“out”)使用监督训练来教攻击模型如何区分影子模型对其训练数据集成员的输出和对非成员的输出。

![[Pasted image 20240713091501.png]]


图1:黑箱设置下的成员推理攻击。攻击者使用数据记录查询目标模型，并获取该记录上的模型预测。预测是一个概率向量，每个类一个，记录属于某个类。该预测向量与目标记录的标签一起传递给攻击模型，攻击模型推断该记录是否在目标模型的训练数据集中。

![[Pasted image 20240713091610.png]]
图2:使用与训练目标模型相同的机器学习平台训练影子模型。目标模型和阴影模型的训练数据集格式相同，但不相交。阴影模型的训练数据集可能重叠。所有模型的内部参数都是独立训练的。
###### 数据生成方法
**Model-based synthesis**
基于模型合成。如果攻击者没有真实的训练数据，也没有任何关于其分布的统计数据，他可以使用目标模型本身为影子模型生成合成的训练数据。直觉是，由目标模型以高置信度分类的记录应该在统计上与目标的训练数据集相似，从而为影子模型提供良好的素材。

合成过程分为两个阶段:(1)搜索可能的数据记录空间，采用爬坡算法寻找被目标模型分类的高置信度输入;(2)从这些记录中取样合成数据。在这个过程合成一条记录后，攻击者可以重复它，直到阴影模型的训练数据集满。

![[Pasted image 20240713091746.png]]

合成过程的伪代码见算法1。首先，修复攻击者想要生成合成数据的类 $c$。第一阶段是一个迭代过程。首先随机初始化一条数据记录 $x$。假设攻击者只知道数据记录的语法格式，从该特征的所有可能值中随机抽取每个特征的值。在每次迭代中，提出一个新记录。只有当提议的记录增加了爬坡目标:被目标模型分类为c类的概率时，才会被接受。 

每次迭代都涉及通过改变最近接受的记录 $x$ 的 $k$ 个随机选择的特征来提出一个新的候选记录。这是通过翻转二进制特征或为其他类型的特征重新绘制新值来完成的。我们初始化 $k$ 为 $k_{\max}$，当后续提案被拒绝时将其除以2。这控制了被接受记录周围的搜索直径，以便提出一个新记录。我们把 $k$ 的最小值设为 $k_{\min}$。这控制了搜索具有潜在更高分类概率 $y_c$ 的新记录的速度。

第二个采样阶段开始于目标模型的概率 $y_c$，即提议的数据记录被归类为属于类别 $c$ 的概率大于所有其他类别的概率，也大于阈值确认。这确保了记录的预测标签是 $c$，并且目标模型对其标签预测有足够的信心。我们以 $y_c^*$ 的概率为合成数据集选择这样的记录，如果选择失败，则重复，直到选择一条记录。

只有当对手能够有效地探索可能输入的空间，并以高置信度发现被目标模型分类的输入时，这种综合过程才有效。例如，如果输入是**高分辨率图像**，并且目标模型执行**复杂的图像分类**任务，则可能不起作用。

**Statistics-based synthesis**
攻击者可能有一些关于群体的统计信息，目标模型的训练数据就是从这些群体中提取出来的。例如，攻击者可能具有**不同特征的边缘分布的先验知识**。在我们的实验中，我们通过从每个特征的边缘分布中独立采样每个特征的值来生成阴影模型的合成训练记录。由此产生的攻击模型非常有效。

**Noisy real data**
攻击者可以访问一些与目标模型的训练数据相似的数据，并且可以将其视为其“噪声”版本。在我们的位置数据集实验中，我们通过翻转10%或20%随机选择的特征的(二进制)值来模拟这一点，然后在得到的噪声数据集上训练我们的阴影模型。这个场景模拟了目标模型和阴影模型的训练数据不是从完全相同的总体中采样，或者以不统一的方式采样的情况。

###### 训练影子模型
![[Pasted image 20240713092735.png]]
图3:在阴影模型的输入输出上训练攻击模型。对于阴影模型训练数据集中的所有记录，我们查询模型并获得输出。这些输出向量被标记为“in”并添加到攻击模型的训练数据集中。我们还查询了与训练数据集不相交的测试数据集的阴影模型。这个集合上的输出被标记为“out”，也被添加到攻击模型的训练数据集中。在构建了一个反映阴影模型在其训练和测试数据集上的黑盒行为的数据集之后，我们训练了一组目标攻击模型，每个目标模型的输出类一个。

For all $(\mathbf x , y) \in {D_{\text{shadow}}^{\text{train}}}_i$ compute the prediction vector $\mathbf y = f_{\text{shadow}}^i(\mathbf x)$ and add the record $(y, \mathbf y, \text{in})$ to the attack training set $(\mathbf x , y) \in {D_{\text{Attack}}^{\text{train}}}$

Let $D_{\text{shadow}}^{\text{train}}$ be a set of records disjoint from the training set of the $i$th shadow model. Then, $\forall (\mathbf x,y) \in {D_{\text{shadow}}^{\text{test}}}_i$ compute the prediction vector $\mathbf y = f_{\text{shadow}}^i (\mathbf x)$ and add the record $(y,\mathbf y,\text{out})$ to the attack training set $D_{\text{attack}}^{\text{train}}$

如果我们使用章节V-C中基于模型的合成，攻击模型的所有原始训练数据都是从目标模型以高置信度分类的记录中提取的。然而，对于影子模型的训练数据集中使用的记录和这些数据集中留下的测试记录来说，这都是正确的。因此，攻击模型并不是**简单地学习识别高置信度分类的输入**。相反，它学习执行一项**更微妙的任务**:如何区分以高置信度分类的**训练输入**和其他同样以高置信度分类的**非训练输入**。

